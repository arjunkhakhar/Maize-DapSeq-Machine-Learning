{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from pandas.io import sql\n",
    "from tables import *\n",
    "import re\n",
    "import pysam\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeSequence(seq):\n",
    "    # the order of the letters is not arbitrary.\n",
    "    # Flip the matrix up-down and left-right for reverse compliment\n",
    "    ltrdict = {'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1], 'n':[0,0,0,0]}\n",
    "    return numpy.array([ltrdict[x] for x in seq])\n",
    "\n",
    "def Generate_training_and_test_datasets(Gem_events_file_path,ARF_label):\n",
    "    \n",
    "    #Make Maize genome\n",
    "    from Bio import SeqIO\n",
    "    for record in SeqIO.parse(open('/mnt/Data_DapSeq_Maize/MaizeGenome.fa'),'fasta'):\n",
    "        if record.id =='1':\n",
    "            chr1 = record.seq.tostring()\n",
    "        if record.id =='2':\n",
    "            chr2 = record.seq.tostring()\n",
    "        if record.id =='3':\n",
    "            chr3 = record.seq.tostring()\n",
    "        if record.id =='4':\n",
    "            chr4 = record.seq.tostring()\n",
    "        if record.id =='5':\n",
    "            chr5 = record.seq.tostring()\n",
    "        if record.id =='6':\n",
    "            chr6 = record.seq.tostring()\n",
    "        if record.id =='7':\n",
    "            chr7 = record.seq.tostring()\n",
    "        if record.id =='8':\n",
    "            chr8 = record.seq.tostring()\n",
    "        if record.id =='9':\n",
    "            chr9 = record.seq.tostring()\n",
    "        if record.id =='10':\n",
    "            chr10 = record.seq.tostring()\n",
    "\n",
    "    wholegenome = {'chr1':chr1,'chr2':chr2,'chr3':chr3,'chr4':chr4,'chr5':chr5,'chr6':chr6,'chr7':chr7,'chr8':chr8,'chr9':chr9,'chr10':chr10}\n",
    "    \n",
    "    \n",
    "    rawdata = open(Gem_events_file_path) \n",
    "    GEM_events=rawdata.read()\n",
    "    GEM_events=re.split(',|\\t|\\n',GEM_events)\n",
    "    GEM_events=GEM_events[0:(len(GEM_events)-1)] # this is to make sure the reshape step works\n",
    "    GEM_events= numpy.reshape(GEM_events,(-1,10))\n",
    "    \n",
    "    #Build Negative dataset\n",
    "    import random\n",
    "    \n",
    "    Bound_Sequences = []\n",
    "    for i in range(0,len(GEM_events)):\n",
    "        Bound_Sequences.append(wholegenome[GEM_events[i][0]][int(GEM_events[i][1]):int(GEM_events[i][2])])\n",
    "    \n",
    "    Un_Bound_Sequences = []\n",
    "    count=0\n",
    "    while count<len(Bound_Sequences):\n",
    "        chro = numpy.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10'])\n",
    "        index = random.randint(1,len(wholegenome[chro]))\n",
    "        absent=True\n",
    "        for i in range(len(GEM_events)):\n",
    "            if chro == GEM_events[i][0]:\n",
    "                if index>int(GEM_events[i][1]) and index<int(GEM_events[i][2]):\n",
    "                    absent = False\n",
    "        if absent:\n",
    "            if wholegenome[chro][index:(index+201)].upper().count('R') == 0 and wholegenome[chro][index:(index+201)].upper().count('W') == 0 and wholegenome[chro][index:(index+201)].upper().count('M') == 0 and wholegenome[chro][index:(index+201)].upper().count('S') == 0 and wholegenome[chro][index:(index+201)].upper().count('K') == 0 and wholegenome[chro][index:(index+201)].upper().count('Y') == 0and wholegenome[chro][index:(index+201)].upper().count('B') == 0 and wholegenome[chro][index:(index+201)].upper().count('D') == 0and wholegenome[chro][index:(index+201)].upper().count('H') == 0 and wholegenome[chro][index:(index+201)].upper().count('V') == 0 and wholegenome[chro][index:(index+201)].upper().count('Z') == 0 and wholegenome[chro][index:(index+201)].upper().count('N') == 0 :\n",
    "                Un_Bound_Sequences.append(wholegenome[chro][index:(index+201)])\n",
    "                count=count+1\n",
    "    response = [0]*(len(Un_Bound_Sequences))\n",
    "    temp3 = numpy.array(Un_Bound_Sequences)\n",
    "    temp2 = numpy.array(response)\n",
    "    neg = pd.DataFrame({'sequence':temp3,'response':temp2})\n",
    "    \n",
    "    #Build Positive dataset labeled with signal value\n",
    "    Bound_Sequences = []\n",
    "    Responses=[]\n",
    "    for i in range(0,len(GEM_events)):\n",
    "        Bound_Sequences.append(wholegenome[GEM_events[i][0]][int(GEM_events[i][1]):int(GEM_events[i][2])])\n",
    "        Responses.append(float(GEM_events[i][6]))\n",
    "    \n",
    "    d = {'sequence' : pd.Series(Bound_Sequences, index=range(len(Bound_Sequences))),\n",
    "        'response' : pd.Series(Responses, index=range(len(Bound_Sequences)))}\n",
    "    pos = pd.DataFrame(d)\n",
    "    \n",
    "    #Put positive and negative datasets together\n",
    "    \n",
    "    LearningData = neg.append(pos)\n",
    "    LearningData = LearningData.reindex()\n",
    "    \n",
    "    #one hot encode sequence data\n",
    "    counter2=0\n",
    "    LearningData_seq_OneHotEncoded =numpy.empty([len(LearningData),201,4])\n",
    "\n",
    "    for counter1 in LearningData['sequence']:\n",
    "        LearningData_seq_OneHotEncoded[counter2]=vectorizeSequence(counter1.lower())\n",
    "        counter2=counter2+1\n",
    "    \n",
    "    #Create training and test datasets\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    sequence_train, sequence_test, response_train, response_test = train_test_split(LearningData_seq_OneHotEncoded, LearningData['response'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    #Saving datasets\n",
    "    \n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'_seq_train.npy',sequence_train)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'_res_train.npy',response_train)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'_seq_test.npy',sequence_test)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'_res_test.npy',response_test)\n",
    "    \n",
    "def Generate_training_and_test_datasets_no_negative(Gem_events_file_path,ARF_label):\n",
    "    \n",
    "    #Make Maize genome\n",
    "    from Bio import SeqIO\n",
    "    for record in SeqIO.parse(open('/mnt/Data_DapSeq_Maize/MaizeGenome.fa'),'fasta'):\n",
    "        if record.id =='1':\n",
    "            chr1 = record.seq.tostring()\n",
    "        if record.id =='2':\n",
    "            chr2 = record.seq.tostring()\n",
    "        if record.id =='3':\n",
    "            chr3 = record.seq.tostring()\n",
    "        if record.id =='4':\n",
    "            chr4 = record.seq.tostring()\n",
    "        if record.id =='5':\n",
    "            chr5 = record.seq.tostring()\n",
    "        if record.id =='6':\n",
    "            chr6 = record.seq.tostring()\n",
    "        if record.id =='7':\n",
    "            chr7 = record.seq.tostring()\n",
    "        if record.id =='8':\n",
    "            chr8 = record.seq.tostring()\n",
    "        if record.id =='9':\n",
    "            chr9 = record.seq.tostring()\n",
    "        if record.id =='10':\n",
    "            chr10 = record.seq.tostring()\n",
    "\n",
    "    wholegenome = {'chr1':chr1,'chr2':chr2,'chr3':chr3,'chr4':chr4,'chr5':chr5,'chr6':chr6,'chr7':chr7,'chr8':chr8,'chr9':chr9,'chr10':chr10}\n",
    "    \n",
    "    \n",
    "    rawdata = open(Gem_events_file_path) \n",
    "    GEM_events=rawdata.read()\n",
    "    GEM_events=re.split(',|\\t|\\n',GEM_events)\n",
    "    GEM_events=GEM_events[0:(len(GEM_events)-1)] # this is to make sure the reshape step works\n",
    "    GEM_events= numpy.reshape(GEM_events,(-1,10))\n",
    "    \n",
    "    #Build Positive dataset labeled with signal value\n",
    "    Bound_Sequences = []\n",
    "    Responses=[]\n",
    "    for i in range(0,len(GEM_events)):\n",
    "        Bound_Sequences.append(wholegenome[GEM_events[i][0]][int(GEM_events[i][1]):int(GEM_events[i][2])])\n",
    "        Responses.append(float(GEM_events[i][6]))\n",
    "    \n",
    "    d = {'sequence' : pd.Series(Bound_Sequences, index=range(len(Bound_Sequences))),\n",
    "        'response' : pd.Series(Responses, index=range(len(Bound_Sequences)))}\n",
    "    pos = pd.DataFrame(d)\n",
    "        \n",
    "    LearningData = pos\n",
    "    \n",
    "    #one hot encode sequence data\n",
    "    counter2=0\n",
    "    LearningData_seq_OneHotEncoded =numpy.empty([len(LearningData),201,4])\n",
    "\n",
    "    for counter1 in LearningData['sequence']:\n",
    "        LearningData_seq_OneHotEncoded[counter2]=vectorizeSequence(counter1.lower())\n",
    "        counter2=counter2+1\n",
    "    \n",
    "    #Create training and test datasets\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    sequence_train, sequence_test, response_train, response_test = train_test_split(LearningData_seq_OneHotEncoded, LearningData['response'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    #Saving datasets\n",
    "    \n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_seq_train.npy',sequence_train)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_res_train.npy',response_train)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_seq_test.npy',sequence_test)\n",
    "    numpy.save('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_res_test.npy',response_test)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Train_and_save_DanQ_model(ARF_label,number_backpropagation_cycles):\n",
    "    \n",
    "    #Loading the data\n",
    "    sequence_train=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'_seq_train.npy')\n",
    "    response_train=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'_res_train.npy')\n",
    "    sequence_test=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'_seq_test.npy')\n",
    "    response_test=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'_res_test.npy')\n",
    "    \n",
    "    #Setting up the model\n",
    "    import keras\n",
    "    import numpy as np\n",
    "    from keras import backend\n",
    "    backend._BACKEND=\"theano\"\n",
    "    \n",
    "    #DanQ model\n",
    "\n",
    "    from keras.preprocessing import sequence\n",
    "    from keras.optimizers import RMSprop\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense\n",
    "    from keras.layers.core import Merge\n",
    "    from keras.layers.core import Dropout\n",
    "    from keras.layers.core import Activation\n",
    "    from keras.layers.core import Flatten\n",
    "    from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "    from keras.regularizers import l2, activity_l1\n",
    "    from keras.constraints import maxnorm\n",
    "    from keras.layers.recurrent import LSTM, GRU\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from keras.layers import Bidirectional\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(nb_filter=20,filter_length=26,input_dim=4,input_length=201,border_mode=\"valid\")) \n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    model.add(MaxPooling1D(pool_length=6, stride=6))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(5))) \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    model.fit(sequence_train, response_train, validation_split=0.2,batch_size=100, nb_epoch=number_backpropagation_cycles, verbose=1)\n",
    "    \n",
    "    #evaulting correlation between model and test data\n",
    "    import scipy\n",
    "\n",
    "    correlation = scipy.stats.pearsonr(response_test,model.predict(sequence_test).flatten())\n",
    "    correlation_2 = (correlation[0]**2)*100\n",
    "\n",
    "    print('Percent of variability explained by model: '+str(correlation_2))\n",
    "    \n",
    "    # saving the model\n",
    "    model.save('/mnt/Data_DapSeq_Maize/TrainedModel_DanQ_' +ARF_label+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Train_and_save_DanQ_model_no_negative(ARF_label,number_backpropagation_cycles,train_size):\n",
    "    \n",
    "    #Loading the data\n",
    "    sequence_train=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_seq_train.npy')\n",
    "    response_train=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_res_train.npy')\n",
    "    sequence_train=sequence_train[0:train_size]\n",
    "    response_train=response_train[0:train_size]\n",
    "    \n",
    "    sequence_test=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_seq_test.npy')\n",
    "    response_test=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_res_test.npy')\n",
    "    \n",
    "    #Setting up the model\n",
    "    import keras\n",
    "    import numpy as np\n",
    "    from keras import backend\n",
    "    backend._BACKEND=\"theano\"\n",
    "    \n",
    "    #DanQ model\n",
    "\n",
    "    from keras.preprocessing import sequence\n",
    "    from keras.optimizers import RMSprop\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense\n",
    "    from keras.layers.core import Merge\n",
    "    from keras.layers.core import Dropout\n",
    "    from keras.layers.core import Activation\n",
    "    from keras.layers.core import Flatten\n",
    "    from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "    from keras.regularizers import l2, activity_l1\n",
    "    from keras.constraints import maxnorm\n",
    "    from keras.layers.recurrent import LSTM, GRU\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from keras.layers import Bidirectional\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(nb_filter=20,filter_length=26,input_dim=4,input_length=201,border_mode=\"valid\")) \n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    model.add(MaxPooling1D(pool_length=6, stride=6))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(5))) \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    \n",
    "    model.fit(sequence_train, response_train, validation_split=0.2,batch_size=100, nb_epoch=number_backpropagation_cycles, verbose=1)\n",
    "    \n",
    "    #evaulting correlation between model and test data\n",
    "    import scipy\n",
    "\n",
    "    correlation = scipy.stats.pearsonr(response_test,model.predict(sequence_test).flatten())\n",
    "    correlation_2 = (correlation[0]**2)*100\n",
    "\n",
    "    print('Percent of variability explained by model: '+str(correlation_2))\n",
    "    \n",
    "    # saving the model\n",
    "    model.save('/mnt/Data_DapSeq_Maize/TrainedModel_DanQ_no_negative_' +ARF_label+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF27_smaller_GEM_events.txt','ARF27_smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF34_smaller_GEM_events.txt','ARF34_smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39335 samples, validate on 9834 samples\n",
      "Epoch 1/35\n",
      "39335/39335 [==============================] - 78s - loss: 5413.9225 - val_loss: 3574.2081\n",
      "Epoch 2/35\n",
      "39335/39335 [==============================] - 77s - loss: 4828.2836 - val_loss: 3371.2509\n",
      "Epoch 3/35\n",
      "39335/39335 [==============================] - 76s - loss: 4700.8217 - val_loss: 3265.7315\n",
      "Epoch 4/35\n",
      "39335/39335 [==============================] - 77s - loss: 4657.4661 - val_loss: 3197.0838\n",
      "Epoch 5/35\n",
      "39335/39335 [==============================] - 76s - loss: 4548.6414 - val_loss: 3018.3132\n",
      "Epoch 6/35\n",
      "39335/39335 [==============================] - 76s - loss: 4383.2111 - val_loss: 3078.2806\n",
      "Epoch 7/35\n",
      "39335/39335 [==============================] - 76s - loss: 4279.4613 - val_loss: 2738.8646\n",
      "Epoch 8/35\n",
      "39335/39335 [==============================] - 76s - loss: 4150.0044 - val_loss: 2653.8492\n",
      "Epoch 9/35\n",
      "39335/39335 [==============================] - 76s - loss: 4099.5382 - val_loss: 2604.9077\n",
      "Epoch 10/35\n",
      "39335/39335 [==============================] - 76s - loss: 4025.9398 - val_loss: 2627.1701\n",
      "Epoch 11/35\n",
      "39335/39335 [==============================] - 76s - loss: 3949.6667 - val_loss: 2499.7585\n",
      "Epoch 12/35\n",
      "39335/39335 [==============================] - 76s - loss: 3885.4647 - val_loss: 2507.1055\n",
      "Epoch 13/35\n",
      "39335/39335 [==============================] - 77s - loss: 3834.0913 - val_loss: 2483.3690\n",
      "Epoch 14/35\n",
      "39335/39335 [==============================] - 77s - loss: 3765.7992 - val_loss: 2629.4725\n",
      "Epoch 15/35\n",
      "39335/39335 [==============================] - 76s - loss: 3745.7723 - val_loss: 2472.9099\n",
      "Epoch 16/35\n",
      "39335/39335 [==============================] - 76s - loss: 3687.3522 - val_loss: 2530.8719\n",
      "Epoch 17/35\n",
      "39335/39335 [==============================] - 76s - loss: 3676.6949 - val_loss: 2498.3089\n",
      "Epoch 18/35\n",
      "39335/39335 [==============================] - 76s - loss: 3592.5604 - val_loss: 2245.5083\n",
      "Epoch 19/35\n",
      "39335/39335 [==============================] - 76s - loss: 3558.8858 - val_loss: 2274.1140\n",
      "Epoch 20/35\n",
      "39335/39335 [==============================] - 76s - loss: 3544.1270 - val_loss: 2195.9600\n",
      "Epoch 21/35\n",
      "39335/39335 [==============================] - 76s - loss: 3513.6770 - val_loss: 2146.6501\n",
      "Epoch 22/35\n",
      "39335/39335 [==============================] - 77s - loss: 3459.1705 - val_loss: 2374.5371\n",
      "Epoch 23/35\n",
      "39335/39335 [==============================] - 76s - loss: 3437.6884 - val_loss: 2113.5236\n",
      "Epoch 24/35\n",
      "39335/39335 [==============================] - 76s - loss: 3401.1535 - val_loss: 2180.6445\n",
      "Epoch 25/35\n",
      "39335/39335 [==============================] - 76s - loss: 3439.9705 - val_loss: 2199.9474\n",
      "Epoch 26/35\n",
      "39335/39335 [==============================] - 76s - loss: 3386.8575 - val_loss: 2127.7529\n",
      "Epoch 27/35\n",
      "39335/39335 [==============================] - 76s - loss: 3375.2390 - val_loss: 2044.7284\n",
      "Epoch 28/35\n",
      "39335/39335 [==============================] - 79s - loss: 3357.9515 - val_loss: 2194.3363\n",
      "Epoch 29/35\n",
      "39335/39335 [==============================] - 80s - loss: 3348.4957 - val_loss: 2234.4569\n",
      "Epoch 30/35\n",
      "39335/39335 [==============================] - 76s - loss: 3334.1245 - val_loss: 2040.7398\n",
      "Epoch 31/35\n",
      "39335/39335 [==============================] - 76s - loss: 3336.5938 - val_loss: 2172.5686\n",
      "Epoch 32/35\n",
      "39335/39335 [==============================] - 77s - loss: 3338.0094 - val_loss: 2039.6627\n",
      "Epoch 33/35\n",
      "39335/39335 [==============================] - 76s - loss: 3310.8657 - val_loss: 2051.2391\n",
      "Epoch 34/35\n",
      "39335/39335 [==============================] - 77s - loss: 3315.2995 - val_loss: 2230.8682\n",
      "Epoch 35/35\n",
      "39335/39335 [==============================] - 76s - loss: 3279.8955 - val_loss: 2103.2891\n",
      "Percent of variability explained by model: 37.9207348375\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF27_smaller',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95597 samples, validate on 23900 samples\n",
      "Epoch 1/35\n",
      "95597/95597 [==============================] - 191s - loss: 4887.8733 - val_loss: 4327.9871\n",
      "Epoch 2/35\n",
      "95597/95597 [==============================] - 190s - loss: 4531.3844 - val_loss: 4251.6972\n",
      "Epoch 3/35\n",
      "95597/95597 [==============================] - 190s - loss: 4456.6051 - val_loss: 4202.6710\n",
      "Epoch 4/35\n",
      "95597/95597 [==============================] - 187s - loss: 4241.0890 - val_loss: 3892.3050\n",
      "Epoch 5/35\n",
      "95597/95597 [==============================] - 186s - loss: 4007.0111 - val_loss: 3519.4219\n",
      "Epoch 6/35\n",
      "95597/95597 [==============================] - 187s - loss: 3848.9529 - val_loss: 3682.6917\n",
      "Epoch 7/35\n",
      "95597/95597 [==============================] - 186s - loss: 3752.3041 - val_loss: 3343.3430\n",
      "Epoch 8/35\n",
      "95597/95597 [==============================] - 187s - loss: 3653.3723 - val_loss: 3197.1191\n",
      "Epoch 9/35\n",
      "95597/95597 [==============================] - 187s - loss: 3575.5975 - val_loss: 3151.9682\n",
      "Epoch 10/35\n",
      "95597/95597 [==============================] - 186s - loss: 3510.2862 - val_loss: 3048.9712\n",
      "Epoch 11/35\n",
      "95597/95597 [==============================] - 187s - loss: 3428.6527 - val_loss: 3085.6631\n",
      "Epoch 12/35\n",
      "95597/95597 [==============================] - 186s - loss: 3327.8164 - val_loss: 2896.9163\n",
      "Epoch 13/35\n",
      "95597/95597 [==============================] - 189s - loss: 3276.2440 - val_loss: 2824.8755\n",
      "Epoch 14/35\n",
      "95597/95597 [==============================] - 195s - loss: 3229.8879 - val_loss: 2941.8023\n",
      "Epoch 15/35\n",
      "95597/95597 [==============================] - 193s - loss: 3187.2784 - val_loss: 2811.2938\n",
      "Epoch 16/35\n",
      "95597/95597 [==============================] - 193s - loss: 3176.0136 - val_loss: 2748.7770\n",
      "Epoch 17/35\n",
      "95597/95597 [==============================] - 194s - loss: 3133.8044 - val_loss: 2899.3654\n",
      "Epoch 18/35\n",
      "95597/95597 [==============================] - 194s - loss: 3113.5332 - val_loss: 2694.3913\n",
      "Epoch 19/35\n",
      "95597/95597 [==============================] - 195s - loss: 3105.4206 - val_loss: 2769.7157\n",
      "Epoch 20/35\n",
      "95597/95597 [==============================] - 196s - loss: 3060.3629 - val_loss: 2691.1610\n",
      "Epoch 21/35\n",
      "95597/95597 [==============================] - 194s - loss: 3052.6832 - val_loss: 2742.2659\n",
      "Epoch 22/35\n",
      "95597/95597 [==============================] - 194s - loss: 3030.7377 - val_loss: 2775.6038\n",
      "Epoch 23/35\n",
      "95597/95597 [==============================] - 195s - loss: 3022.7071 - val_loss: 2813.7917\n",
      "Epoch 24/35\n",
      "95597/95597 [==============================] - 194s - loss: 3019.5164 - val_loss: 2646.4062\n",
      "Epoch 25/35\n",
      "95597/95597 [==============================] - 190s - loss: 3012.5687 - val_loss: 2646.8621\n",
      "Epoch 26/35\n",
      "95597/95597 [==============================] - 185s - loss: 2996.6075 - val_loss: 2636.6273\n",
      "Epoch 27/35\n",
      "95597/95597 [==============================] - 194s - loss: 2996.8960 - val_loss: 2693.1497\n",
      "Epoch 28/35\n",
      "95597/95597 [==============================] - 190s - loss: 2988.2616 - val_loss: 2636.2069\n",
      "Epoch 29/35\n",
      "95597/95597 [==============================] - 186s - loss: 2978.7858 - val_loss: 2723.5984\n",
      "Epoch 30/35\n",
      "95597/95597 [==============================] - 185s - loss: 2979.4342 - val_loss: 2679.8599\n",
      "Epoch 31/35\n",
      "95597/95597 [==============================] - 190s - loss: 2975.5008 - val_loss: 2696.6434\n",
      "Epoch 32/35\n",
      "95597/95597 [==============================] - 195s - loss: 2944.4941 - val_loss: 2642.6848\n",
      "Epoch 33/35\n",
      "95597/95597 [==============================] - 195s - loss: 2965.7662 - val_loss: 2626.8915\n",
      "Epoch 34/35\n",
      "95597/95597 [==============================] - 195s - loss: 2954.4101 - val_loss: 2654.3864\n",
      "Epoch 35/35\n",
      "95597/95597 [==============================] - 196s - loss: 2939.8508 - val_loss: 2596.2920\n",
      "Percent of variability explained by model: 39.7201759078\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF34_smaller',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF16_GEM_events.txt','ARF16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF4_GEM_events.txt','ARF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF4_rep2_GEM_events.txt','ARF4_rep2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF4_rep3_GEM_events.txt','ARF4_rep3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF10_GEM_events.txt','ARF10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF13_GEM_events.txt','ARF13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF18_GEM_events.txt','ARF18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF27_GEM_events.txt','ARF27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF29_GEM_events.txt','ARF29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF34_GEM_events.txt','ARF34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF35_GEM_events.txt','ARF35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF39_GEM_events.txt','ARF39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF10_rep1_ear_GEM_events.txt','ARF10_rep1_ear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF10_rep2_ear_GEM_events.txt','ARF10_rep2_ear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF10_rep1_tassel_GEM_events.txt','ARF10_rep1_tassel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF10_rep2_tassel_GEM_events.txt','ARF10_rep2_tassel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF7_GEM_events.txt','ARF7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF14_GEM_events.txt','ARF14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF24_GEM_events.txt','ARF24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF25_GEM_events.txt','ARF25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets('/mnt/Data_DapSeq_Maize/ARF36_GEM_events.txt','ARF36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37622 samples, validate on 9406 samples\n",
      "Epoch 1/35\n",
      "37622/37622 [==============================] - 75s - loss: 1642.5587 - val_loss: 1406.0280\n",
      "Epoch 2/35\n",
      "37622/37622 [==============================] - 73s - loss: 1336.9557 - val_loss: 1212.2301\n",
      "Epoch 3/35\n",
      "37622/37622 [==============================] - 73s - loss: 1176.2723 - val_loss: 1150.1994\n",
      "Epoch 4/35\n",
      "37622/37622 [==============================] - 73s - loss: 1108.3225 - val_loss: 1029.0691\n",
      "Epoch 5/35\n",
      "37622/37622 [==============================] - 73s - loss: 1028.1324 - val_loss: 947.3139\n",
      "Epoch 6/35\n",
      "37622/37622 [==============================] - 73s - loss: 965.0899 - val_loss: 875.0999\n",
      "Epoch 7/35\n",
      "37622/37622 [==============================] - 73s - loss: 901.5572 - val_loss: 870.0822\n",
      "Epoch 8/35\n",
      "37622/37622 [==============================] - 73s - loss: 856.4073 - val_loss: 826.4472\n",
      "Epoch 9/35\n",
      "37622/37622 [==============================] - 72s - loss: 819.4912 - val_loss: 783.1696\n",
      "Epoch 10/35\n",
      "37622/37622 [==============================] - 72s - loss: 784.3695 - val_loss: 760.3283\n",
      "Epoch 11/35\n",
      "37622/37622 [==============================] - 72s - loss: 758.6375 - val_loss: 729.9284\n",
      "Epoch 12/35\n",
      "37622/37622 [==============================] - 71s - loss: 745.9593 - val_loss: 749.6967\n",
      "Epoch 13/35\n",
      "37622/37622 [==============================] - 71s - loss: 721.7378 - val_loss: 781.7918\n",
      "Epoch 14/35\n",
      "37622/37622 [==============================] - 71s - loss: 718.1789 - val_loss: 697.5892\n",
      "Epoch 15/35\n",
      "37622/37622 [==============================] - 71s - loss: 706.1188 - val_loss: 677.1304\n",
      "Epoch 16/35\n",
      "37622/37622 [==============================] - 71s - loss: 694.4273 - val_loss: 708.1663\n",
      "Epoch 17/35\n",
      "37622/37622 [==============================] - 71s - loss: 690.0454 - val_loss: 682.0459\n",
      "Epoch 18/35\n",
      "37622/37622 [==============================] - 71s - loss: 674.9655 - val_loss: 662.0489\n",
      "Epoch 19/35\n",
      "37622/37622 [==============================] - 76s - loss: 666.9247 - val_loss: 697.7107\n",
      "Epoch 20/35\n",
      "37622/37622 [==============================] - 75s - loss: 674.5480 - val_loss: 669.1659\n",
      "Epoch 21/35\n",
      "37622/37622 [==============================] - 76s - loss: 663.2988 - val_loss: 663.8743\n",
      "Epoch 22/35\n",
      "37622/37622 [==============================] - 75s - loss: 659.1365 - val_loss: 657.6840\n",
      "Epoch 23/35\n",
      "37622/37622 [==============================] - 75s - loss: 654.2977 - val_loss: 649.1570\n",
      "Epoch 24/35\n",
      "37622/37622 [==============================] - 76s - loss: 643.8327 - val_loss: 677.3617\n",
      "Epoch 25/35\n",
      "37622/37622 [==============================] - 75s - loss: 646.6610 - val_loss: 663.5896\n",
      "Epoch 26/35\n",
      "37622/37622 [==============================] - 71s - loss: 645.0253 - val_loss: 637.0758\n",
      "Epoch 27/35\n",
      "37622/37622 [==============================] - 72s - loss: 642.7390 - val_loss: 757.2545\n",
      "Epoch 28/35\n",
      "37622/37622 [==============================] - 75s - loss: 636.6063 - val_loss: 640.7335\n",
      "Epoch 29/35\n",
      "37622/37622 [==============================] - 76s - loss: 637.7420 - val_loss: 631.9771\n",
      "Epoch 30/35\n",
      "37622/37622 [==============================] - 75s - loss: 634.9003 - val_loss: 703.8892\n",
      "Epoch 31/35\n",
      "37622/37622 [==============================] - 76s - loss: 632.8757 - val_loss: 700.1804\n",
      "Epoch 32/35\n",
      "37622/37622 [==============================] - 75s - loss: 622.5496 - val_loss: 696.3796\n",
      "Epoch 33/35\n",
      "37622/37622 [==============================] - 75s - loss: 623.5021 - val_loss: 665.2572\n",
      "Epoch 34/35\n",
      "37622/37622 [==============================] - 75s - loss: 624.5120 - val_loss: 632.5013\n",
      "Epoch 35/35\n",
      "37622/37622 [==============================] - 76s - loss: 623.1072 - val_loss: 633.4261\n",
      "Percent of variability explained by model: 59.0605800982\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF7',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35964 samples, validate on 8992 samples\n",
      "Epoch 1/35\n",
      "35964/35964 [==============================] - 74s - loss: 11727.8352 - val_loss: 10908.0097\n",
      "Epoch 2/35\n",
      "35964/35964 [==============================] - 74s - loss: 10596.9515 - val_loss: 9942.7811\n",
      "Epoch 3/35\n",
      "35964/35964 [==============================] - 73s - loss: 10001.3984 - val_loss: 9175.8203\n",
      "Epoch 4/35\n",
      "35964/35964 [==============================] - 73s - loss: 9506.0265 - val_loss: 8702.6930\n",
      "Epoch 5/35\n",
      "35964/35964 [==============================] - 73s - loss: 9059.9357 - val_loss: 8096.7627\n",
      "Epoch 6/35\n",
      "35964/35964 [==============================] - 73s - loss: 8630.9983 - val_loss: 7631.9033\n",
      "Epoch 7/35\n",
      "35964/35964 [==============================] - 72s - loss: 8143.9787 - val_loss: 7030.0634\n",
      "Epoch 8/35\n",
      "35964/35964 [==============================] - 72s - loss: 7883.4281 - val_loss: 7317.5209\n",
      "Epoch 9/35\n",
      "35964/35964 [==============================] - 72s - loss: 7467.0412 - val_loss: 6484.9748\n",
      "Epoch 10/35\n",
      "35964/35964 [==============================] - 72s - loss: 7209.5031 - val_loss: 5997.7775\n",
      "Epoch 11/35\n",
      "35964/35964 [==============================] - 72s - loss: 6927.5920 - val_loss: 5788.0981\n",
      "Epoch 12/35\n",
      "35964/35964 [==============================] - 72s - loss: 6715.3237 - val_loss: 5478.6077\n",
      "Epoch 13/35\n",
      "35964/35964 [==============================] - 72s - loss: 6533.4871 - val_loss: 5379.8309\n",
      "Epoch 14/35\n",
      "35964/35964 [==============================] - 72s - loss: 6399.8846 - val_loss: 5217.7689\n",
      "Epoch 15/35\n",
      "35964/35964 [==============================] - 72s - loss: 6250.6924 - val_loss: 5029.8046\n",
      "Epoch 16/35\n",
      "35964/35964 [==============================] - 72s - loss: 5975.3661 - val_loss: 5159.6967\n",
      "Epoch 17/35\n",
      "35964/35964 [==============================] - 72s - loss: 5975.6800 - val_loss: 4699.4047\n",
      "Epoch 18/35\n",
      "35964/35964 [==============================] - 72s - loss: 5835.0874 - val_loss: 4610.2140\n",
      "Epoch 19/35\n",
      "35964/35964 [==============================] - 72s - loss: 5735.6967 - val_loss: 4534.0169\n",
      "Epoch 20/35\n",
      "35964/35964 [==============================] - 72s - loss: 5720.0174 - val_loss: 5235.4754\n",
      "Epoch 21/35\n",
      "35964/35964 [==============================] - 72s - loss: 5552.5980 - val_loss: 4426.9628\n",
      "Epoch 22/35\n",
      "35964/35964 [==============================] - 72s - loss: 5475.6967 - val_loss: 4795.1638\n",
      "Epoch 23/35\n",
      "35964/35964 [==============================] - 73s - loss: 5432.1412 - val_loss: 4303.8051\n",
      "Epoch 24/35\n",
      "35964/35964 [==============================] - 73s - loss: 5307.4175 - val_loss: 4795.9356\n",
      "Epoch 25/35\n",
      "35964/35964 [==============================] - 72s - loss: 5139.6907 - val_loss: 4397.4290\n",
      "Epoch 26/35\n",
      "35964/35964 [==============================] - 73s - loss: 5145.8064 - val_loss: 4000.1930\n",
      "Epoch 27/35\n",
      "35964/35964 [==============================] - 72s - loss: 5211.1155 - val_loss: 4097.8988\n",
      "Epoch 28/35\n",
      "35964/35964 [==============================] - 72s - loss: 5160.3637 - val_loss: 3984.9019\n",
      "Epoch 29/35\n",
      "35964/35964 [==============================] - 72s - loss: 5035.2585 - val_loss: 4935.7730\n",
      "Epoch 30/35\n",
      "35964/35964 [==============================] - 73s - loss: 4914.4231 - val_loss: 4161.9653\n",
      "Epoch 31/35\n",
      "35964/35964 [==============================] - 73s - loss: 4946.0091 - val_loss: 4002.8170\n",
      "Epoch 32/35\n",
      "35964/35964 [==============================] - 72s - loss: 4824.1349 - val_loss: 4342.5013\n",
      "Epoch 33/35\n",
      "35964/35964 [==============================] - 73s - loss: 4855.1827 - val_loss: 3785.8531\n",
      "Epoch 34/35\n",
      "35964/35964 [==============================] - 73s - loss: 4894.3338 - val_loss: 4307.0775\n",
      "Epoch 35/35\n",
      "35964/35964 [==============================] - 73s - loss: 4780.3014 - val_loss: 3909.9706\n",
      "Percent of variability explained by model: 57.1741803243\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF14',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6246 samples, validate on 1562 samples\n",
      "Epoch 1/35\n",
      "6246/6246 [==============================] - 13s - loss: 1845.6847 - val_loss: 1268.6567\n",
      "Epoch 2/35\n",
      "6246/6246 [==============================] - 13s - loss: 1752.4039 - val_loss: 1191.2670\n",
      "Epoch 3/35\n",
      "6246/6246 [==============================] - 13s - loss: 1664.3215 - val_loss: 1105.1476\n",
      "Epoch 4/35\n",
      "6246/6246 [==============================] - 13s - loss: 1575.9163 - val_loss: 1012.8874\n",
      "Epoch 5/35\n",
      "6246/6246 [==============================] - 13s - loss: 1481.7557 - val_loss: 927.2195\n",
      "Epoch 6/35\n",
      "6246/6246 [==============================] - 13s - loss: 1399.2385 - val_loss: 860.5576\n",
      "Epoch 7/35\n",
      "6246/6246 [==============================] - 13s - loss: 1359.1820 - val_loss: 819.0821\n",
      "Epoch 8/35\n",
      "6246/6246 [==============================] - 12s - loss: 1329.2209 - val_loss: 803.4981\n",
      "Epoch 9/35\n",
      "6246/6246 [==============================] - 13s - loss: 1322.7034 - val_loss: 799.4066\n",
      "Epoch 10/35\n",
      "6246/6246 [==============================] - 13s - loss: 1316.4996 - val_loss: 798.5440\n",
      "Epoch 11/35\n",
      "6246/6246 [==============================] - 13s - loss: 1313.0510 - val_loss: 786.2240\n",
      "Epoch 12/35\n",
      "6246/6246 [==============================] - 13s - loss: 1283.1800 - val_loss: 765.1199\n",
      "Epoch 13/35\n",
      "6246/6246 [==============================] - 13s - loss: 1266.8248 - val_loss: 756.6198\n",
      "Epoch 14/35\n",
      "6246/6246 [==============================] - 12s - loss: 1256.1989 - val_loss: 707.6280\n",
      "Epoch 15/35\n",
      "6246/6246 [==============================] - 13s - loss: 1230.8148 - val_loss: 726.1959\n",
      "Epoch 16/35\n",
      "6246/6246 [==============================] - 13s - loss: 1215.7759 - val_loss: 677.1362\n",
      "Epoch 17/35\n",
      "6246/6246 [==============================] - 13s - loss: 1221.3784 - val_loss: 663.0693\n",
      "Epoch 18/35\n",
      "6246/6246 [==============================] - 13s - loss: 1201.1836 - val_loss: 663.7076\n",
      "Epoch 19/35\n",
      "6246/6246 [==============================] - 13s - loss: 1202.6870 - val_loss: 667.0089\n",
      "Epoch 20/35\n",
      "6246/6246 [==============================] - 13s - loss: 1186.6280 - val_loss: 749.2782\n",
      "Epoch 21/35\n",
      "6246/6246 [==============================] - 13s - loss: 1172.2532 - val_loss: 596.5781\n",
      "Epoch 22/35\n",
      "6246/6246 [==============================] - 13s - loss: 1155.1508 - val_loss: 659.9744\n",
      "Epoch 23/35\n",
      "6246/6246 [==============================] - 13s - loss: 1141.3941 - val_loss: 551.5812\n",
      "Epoch 24/35\n",
      "6246/6246 [==============================] - 13s - loss: 1124.9158 - val_loss: 732.5192\n",
      "Epoch 25/35\n",
      "6246/6246 [==============================] - 12s - loss: 1122.6277 - val_loss: 612.5024\n",
      "Epoch 26/35\n",
      "6246/6246 [==============================] - 13s - loss: 1085.2072 - val_loss: 662.0493\n",
      "Epoch 27/35\n",
      "6246/6246 [==============================] - 13s - loss: 1088.8146 - val_loss: 596.4913\n",
      "Epoch 28/35\n",
      "6246/6246 [==============================] - 13s - loss: 1081.4676 - val_loss: 644.7216\n",
      "Epoch 29/35\n",
      "6246/6246 [==============================] - 12s - loss: 1068.0843 - val_loss: 577.8746\n",
      "Epoch 30/35\n",
      "6246/6246 [==============================] - 13s - loss: 1064.6992 - val_loss: 507.9213\n",
      "Epoch 31/35\n",
      "6246/6246 [==============================] - 12s - loss: 1042.6208 - val_loss: 558.5735\n",
      "Epoch 32/35\n",
      "6246/6246 [==============================] - 12s - loss: 1038.6201 - val_loss: 462.0628\n",
      "Epoch 33/35\n",
      "6246/6246 [==============================] - 12s - loss: 1021.2140 - val_loss: 551.6287\n",
      "Epoch 34/35\n",
      "6246/6246 [==============================] - 12s - loss: 1033.1250 - val_loss: 720.6261\n",
      "Epoch 35/35\n",
      "6246/6246 [==============================] - 12s - loss: 1021.9171 - val_loss: 574.5370\n",
      "Percent of variability explained by model: 26.8260213126\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF24',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49043 samples, validate on 12261 samples\n",
      "Epoch 1/35\n",
      "49043/49043 [==============================] - 100s - loss: 4231.9249 - val_loss: 3410.6344\n",
      "Epoch 2/35\n",
      "49043/49043 [==============================] - 99s - loss: 3469.8376 - val_loss: 2913.4720\n",
      "Epoch 3/35\n",
      "49043/49043 [==============================] - 99s - loss: 2966.6209 - val_loss: 2344.9716\n",
      "Epoch 4/35\n",
      "49043/49043 [==============================] - 99s - loss: 2545.7802 - val_loss: 2041.7218\n",
      "Epoch 5/35\n",
      "49043/49043 [==============================] - 99s - loss: 2342.5778 - val_loss: 1881.4029\n",
      "Epoch 6/35\n",
      "49043/49043 [==============================] - 99s - loss: 2248.5204 - val_loss: 1823.6334\n",
      "Epoch 7/35\n",
      "49043/49043 [==============================] - 100s - loss: 2156.5479 - val_loss: 1767.9331\n",
      "Epoch 8/35\n",
      "49043/49043 [==============================] - 99s - loss: 2086.5730 - val_loss: 1767.0500\n",
      "Epoch 9/35\n",
      "49043/49043 [==============================] - 98s - loss: 2031.4524 - val_loss: 1708.2211\n",
      "Epoch 10/35\n",
      "49043/49043 [==============================] - 99s - loss: 2004.6410 - val_loss: 1719.0536\n",
      "Epoch 11/35\n",
      "49043/49043 [==============================] - 99s - loss: 1973.2102 - val_loss: 1636.6358\n",
      "Epoch 12/35\n",
      "49043/49043 [==============================] - 99s - loss: 1905.9724 - val_loss: 1592.5196\n",
      "Epoch 13/35\n",
      "49043/49043 [==============================] - 99s - loss: 1880.8204 - val_loss: 1581.3771\n",
      "Epoch 14/35\n",
      "49043/49043 [==============================] - 99s - loss: 1830.4650 - val_loss: 1497.1677\n",
      "Epoch 15/35\n",
      "49043/49043 [==============================] - 99s - loss: 1772.0954 - val_loss: 1643.0501\n",
      "Epoch 16/35\n",
      "49043/49043 [==============================] - 99s - loss: 1744.7997 - val_loss: 1477.3018\n",
      "Epoch 17/35\n",
      "49043/49043 [==============================] - 99s - loss: 1704.1094 - val_loss: 1537.0124\n",
      "Epoch 18/35\n",
      "49043/49043 [==============================] - 99s - loss: 1700.3115 - val_loss: 1378.1721\n",
      "Epoch 19/35\n",
      "49043/49043 [==============================] - 99s - loss: 1657.9233 - val_loss: 1330.4318\n",
      "Epoch 20/35\n",
      "49043/49043 [==============================] - 99s - loss: 1645.3381 - val_loss: 1332.2371\n",
      "Epoch 21/35\n",
      "49043/49043 [==============================] - 99s - loss: 1638.3268 - val_loss: 1262.7675\n",
      "Epoch 22/35\n",
      "49043/49043 [==============================] - 99s - loss: 1611.4036 - val_loss: 1268.2791\n",
      "Epoch 23/35\n",
      "49043/49043 [==============================] - 99s - loss: 1604.3058 - val_loss: 1657.9262\n",
      "Epoch 24/35\n",
      "49043/49043 [==============================] - 99s - loss: 1575.7680 - val_loss: 1271.7683\n",
      "Epoch 25/35\n",
      "49043/49043 [==============================] - 99s - loss: 1573.3003 - val_loss: 1349.9233\n",
      "Epoch 26/35\n",
      "49043/49043 [==============================] - 99s - loss: 1558.2760 - val_loss: 1800.3849\n",
      "Epoch 27/35\n",
      "49043/49043 [==============================] - 99s - loss: 1552.2584 - val_loss: 1258.8138\n",
      "Epoch 28/35\n",
      "49043/49043 [==============================] - 99s - loss: 1552.2192 - val_loss: 1246.9384\n",
      "Epoch 29/35\n",
      "49043/49043 [==============================] - 99s - loss: 1550.2047 - val_loss: 1262.9190\n",
      "Epoch 30/35\n",
      "49043/49043 [==============================] - 99s - loss: 1522.2117 - val_loss: 1233.3699\n",
      "Epoch 31/35\n",
      "49043/49043 [==============================] - 99s - loss: 1540.4941 - val_loss: 1257.5926\n",
      "Epoch 32/35\n",
      "49043/49043 [==============================] - 99s - loss: 1495.7793 - val_loss: 1220.7274\n",
      "Epoch 33/35\n",
      "49043/49043 [==============================] - 99s - loss: 1493.1641 - val_loss: 1446.7592\n",
      "Epoch 34/35\n",
      "49043/49043 [==============================] - 99s - loss: 1496.3455 - val_loss: 1301.2476\n",
      "Epoch 35/35\n",
      "49043/49043 [==============================] - 99s - loss: 1474.7239 - val_loss: 1228.7005\n",
      "Percent of variability explained by model: 68.9232006375\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF25',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27740 samples, validate on 6936 samples\n",
      "Epoch 1/35\n",
      "27740/27740 [==============================] - 57s - loss: 2706.4706 - val_loss: 2243.1777\n",
      "Epoch 2/35\n",
      "27740/27740 [==============================] - 56s - loss: 2298.0900 - val_loss: 1827.3086\n",
      "Epoch 3/35\n",
      "27740/27740 [==============================] - 56s - loss: 1968.8071 - val_loss: 1543.3805\n",
      "Epoch 4/35\n",
      "27740/27740 [==============================] - 56s - loss: 1780.6917 - val_loss: 1374.7958\n",
      "Epoch 5/35\n",
      "27740/27740 [==============================] - 56s - loss: 1633.8010 - val_loss: 1190.2560\n",
      "Epoch 6/35\n",
      "27740/27740 [==============================] - 56s - loss: 1469.4236 - val_loss: 1080.1638\n",
      "Epoch 7/35\n",
      "27740/27740 [==============================] - 56s - loss: 1360.2485 - val_loss: 938.3718\n",
      "Epoch 8/35\n",
      "27740/27740 [==============================] - 57s - loss: 1276.4778 - val_loss: 1320.8241\n",
      "Epoch 9/35\n",
      "27740/27740 [==============================] - 56s - loss: 1222.6943 - val_loss: 962.0015\n",
      "Epoch 10/35\n",
      "27740/27740 [==============================] - 57s - loss: 1169.1075 - val_loss: 777.1483\n",
      "Epoch 11/35\n",
      "27740/27740 [==============================] - 56s - loss: 1119.6554 - val_loss: 755.8332\n",
      "Epoch 12/35\n",
      "27740/27740 [==============================] - 56s - loss: 1096.4447 - val_loss: 724.9508\n",
      "Epoch 13/35\n",
      "27740/27740 [==============================] - 56s - loss: 1061.7248 - val_loss: 697.2680\n",
      "Epoch 14/35\n",
      "27740/27740 [==============================] - 55s - loss: 1048.8343 - val_loss: 681.8855\n",
      "Epoch 15/35\n",
      "27740/27740 [==============================] - 55s - loss: 1005.5454 - val_loss: 671.5647\n",
      "Epoch 16/35\n",
      "27740/27740 [==============================] - 56s - loss: 982.8432 - val_loss: 689.3844\n",
      "Epoch 17/35\n",
      "27740/27740 [==============================] - 55s - loss: 966.7395 - val_loss: 667.2142\n",
      "Epoch 18/35\n",
      "27740/27740 [==============================] - 55s - loss: 956.8186 - val_loss: 637.4529\n",
      "Epoch 19/35\n",
      "27740/27740 [==============================] - 55s - loss: 920.2142 - val_loss: 684.8309\n",
      "Epoch 20/35\n",
      "27740/27740 [==============================] - 55s - loss: 910.9738 - val_loss: 625.5196\n",
      "Epoch 21/35\n",
      "27740/27740 [==============================] - 55s - loss: 892.5888 - val_loss: 558.9853\n",
      "Epoch 22/35\n",
      "27740/27740 [==============================] - 56s - loss: 880.0646 - val_loss: 546.9863\n",
      "Epoch 23/35\n",
      "27740/27740 [==============================] - 55s - loss: 871.3335 - val_loss: 549.5048\n",
      "Epoch 24/35\n",
      "27740/27740 [==============================] - 55s - loss: 843.2825 - val_loss: 682.5569\n",
      "Epoch 25/35\n",
      "27740/27740 [==============================] - 56s - loss: 845.7665 - val_loss: 578.4210\n",
      "Epoch 26/35\n",
      "27740/27740 [==============================] - 55s - loss: 839.3233 - val_loss: 523.9775\n",
      "Epoch 27/35\n",
      "27740/27740 [==============================] - 56s - loss: 844.6519 - val_loss: 549.6877\n",
      "Epoch 28/35\n",
      "27740/27740 [==============================] - 55s - loss: 824.7393 - val_loss: 581.6711\n",
      "Epoch 29/35\n",
      "27740/27740 [==============================] - 56s - loss: 816.1288 - val_loss: 610.2768\n",
      "Epoch 30/35\n",
      "27740/27740 [==============================] - 56s - loss: 822.5226 - val_loss: 501.3240\n",
      "Epoch 31/35\n",
      "27740/27740 [==============================] - 55s - loss: 806.5089 - val_loss: 524.9157\n",
      "Epoch 32/35\n",
      "27740/27740 [==============================] - 56s - loss: 803.0831 - val_loss: 766.9125\n",
      "Epoch 33/35\n",
      "27740/27740 [==============================] - 56s - loss: 777.0254 - val_loss: 494.8405\n",
      "Epoch 34/35\n",
      "27740/27740 [==============================] - 56s - loss: 794.8929 - val_loss: 509.8121\n",
      "Epoch 35/35\n",
      "27740/27740 [==============================] - 56s - loss: 769.7662 - val_loss: 509.3959\n",
      "Percent of variability explained by model: 53.5808931124\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF36',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36341 samples, validate on 9086 samples\n",
      "Epoch 1/35\n",
      "36341/36341 [==============================] - 73s - loss: 2002.8636 - val_loss: 1671.7916\n",
      "Epoch 2/35\n",
      "36341/36341 [==============================] - 71s - loss: 1504.8027 - val_loss: 1196.3358\n",
      "Epoch 3/35\n",
      "36341/36341 [==============================] - 71s - loss: 1209.3262 - val_loss: 1019.3311\n",
      "Epoch 4/35\n",
      "36341/36341 [==============================] - 73s - loss: 1092.2851 - val_loss: 859.4826\n",
      "Epoch 5/35\n",
      "36341/36341 [==============================] - 72s - loss: 978.7255 - val_loss: 724.9203\n",
      "Epoch 6/35\n",
      "36341/36341 [==============================] - 72s - loss: 913.3822 - val_loss: 719.9395\n",
      "Epoch 7/35\n",
      "36341/36341 [==============================] - 71s - loss: 875.5293 - val_loss: 673.0429\n",
      "Epoch 8/35\n",
      "36341/36341 [==============================] - 72s - loss: 844.2221 - val_loss: 643.7188\n",
      "Epoch 9/35\n",
      "36341/36341 [==============================] - 72s - loss: 815.6348 - val_loss: 663.3058\n",
      "Epoch 10/35\n",
      "36341/36341 [==============================] - 71s - loss: 806.2997 - val_loss: 596.2601\n",
      "Epoch 11/35\n",
      "36341/36341 [==============================] - 72s - loss: 768.3172 - val_loss: 584.8339\n",
      "Epoch 12/35\n",
      "36341/36341 [==============================] - 72s - loss: 748.4000 - val_loss: 571.5104\n",
      "Epoch 13/35\n",
      "36341/36341 [==============================] - 72s - loss: 733.1704 - val_loss: 589.8676\n",
      "Epoch 14/35\n",
      "36341/36341 [==============================] - 72s - loss: 718.6873 - val_loss: 550.3388\n",
      "Epoch 15/35\n",
      "36341/36341 [==============================] - 72s - loss: 708.7665 - val_loss: 526.1984\n",
      "Epoch 16/35\n",
      "36341/36341 [==============================] - 72s - loss: 691.5063 - val_loss: 516.0198\n",
      "Epoch 17/35\n",
      "36341/36341 [==============================] - 72s - loss: 677.4207 - val_loss: 503.8839\n",
      "Epoch 18/35\n",
      "36341/36341 [==============================] - 72s - loss: 665.8223 - val_loss: 500.0983\n",
      "Epoch 19/35\n",
      "36341/36341 [==============================] - 72s - loss: 657.6152 - val_loss: 493.0187\n",
      "Epoch 20/35\n",
      "36341/36341 [==============================] - 71s - loss: 648.6131 - val_loss: 482.3128\n",
      "Epoch 21/35\n",
      "36341/36341 [==============================] - 71s - loss: 644.3441 - val_loss: 600.8139\n",
      "Epoch 22/35\n",
      "36341/36341 [==============================] - 72s - loss: 632.0800 - val_loss: 492.0201\n",
      "Epoch 23/35\n",
      "36341/36341 [==============================] - 72s - loss: 623.4459 - val_loss: 502.1225\n",
      "Epoch 24/35\n",
      "36341/36341 [==============================] - 72s - loss: 613.6527 - val_loss: 457.0563\n",
      "Epoch 25/35\n",
      "36341/36341 [==============================] - 72s - loss: 609.7626 - val_loss: 517.6812\n",
      "Epoch 26/35\n",
      "36341/36341 [==============================] - 72s - loss: 602.9355 - val_loss: 469.3075\n",
      "Epoch 27/35\n",
      "36341/36341 [==============================] - 71s - loss: 593.9873 - val_loss: 465.8076\n",
      "Epoch 28/35\n",
      "36341/36341 [==============================] - 72s - loss: 587.8732 - val_loss: 450.8730\n",
      "Epoch 29/35\n",
      "36341/36341 [==============================] - 72s - loss: 585.9450 - val_loss: 436.0367\n",
      "Epoch 30/35\n",
      "36341/36341 [==============================] - 72s - loss: 580.2280 - val_loss: 464.4664\n",
      "Epoch 31/35\n",
      "36341/36341 [==============================] - 70s - loss: 578.0275 - val_loss: 432.5083\n",
      "Epoch 32/35\n",
      "36341/36341 [==============================] - 70s - loss: 570.0498 - val_loss: 498.9950\n",
      "Epoch 33/35\n",
      "36341/36341 [==============================] - 69s - loss: 576.3021 - val_loss: 441.4339\n",
      "Epoch 34/35\n",
      "36341/36341 [==============================] - 69s - loss: 558.1740 - val_loss: 491.2429\n",
      "Epoch 35/35\n",
      "36341/36341 [==============================] - 70s - loss: 561.8671 - val_loss: 521.1306\n",
      "Percent of variability explained by model: 68.5095567365\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF10_rep1_ear',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53817 samples, validate on 13455 samples\n",
      "Epoch 1/35\n",
      "53817/53817 [==============================] - 105s - loss: 2735.9892 - val_loss: 2196.0429\n",
      "Epoch 2/35\n",
      "53817/53817 [==============================] - 104s - loss: 2074.7095 - val_loss: 1679.8025\n",
      "Epoch 3/35\n",
      "53817/53817 [==============================] - 105s - loss: 1823.5133 - val_loss: 1452.7318\n",
      "Epoch 4/35\n",
      "53817/53817 [==============================] - 106s - loss: 1548.3769 - val_loss: 1139.5833\n",
      "Epoch 5/35\n",
      "53817/53817 [==============================] - 107s - loss: 1368.5312 - val_loss: 1367.0449\n",
      "Epoch 6/35\n",
      "53817/53817 [==============================] - 106s - loss: 1270.4029 - val_loss: 956.2281\n",
      "Epoch 7/35\n",
      "53817/53817 [==============================] - 107s - loss: 1224.3185 - val_loss: 918.1251\n",
      "Epoch 8/35\n",
      "53817/53817 [==============================] - 107s - loss: 1185.7815 - val_loss: 918.5137\n",
      "Epoch 9/35\n",
      "53817/53817 [==============================] - 106s - loss: 1151.5525 - val_loss: 851.4569\n",
      "Epoch 10/35\n",
      "53817/53817 [==============================] - 108s - loss: 1127.1724 - val_loss: 974.0705\n",
      "Epoch 11/35\n",
      "53817/53817 [==============================] - 107s - loss: 1096.5490 - val_loss: 1210.4117\n",
      "Epoch 12/35\n",
      "53817/53817 [==============================] - 106s - loss: 1084.2597 - val_loss: 888.9740\n",
      "Epoch 13/35\n",
      "53817/53817 [==============================] - 105s - loss: 1060.1046 - val_loss: 800.3079\n",
      "Epoch 14/35\n",
      "53817/53817 [==============================] - 105s - loss: 1031.4883 - val_loss: 768.4859\n",
      "Epoch 15/35\n",
      "53817/53817 [==============================] - 107s - loss: 1017.6233 - val_loss: 833.3002\n",
      "Epoch 16/35\n",
      "53817/53817 [==============================] - 105s - loss: 999.5198 - val_loss: 959.5333\n",
      "Epoch 17/35\n",
      "53817/53817 [==============================] - 107s - loss: 994.9775 - val_loss: 768.8517\n",
      "Epoch 18/35\n",
      "53817/53817 [==============================] - 108s - loss: 966.5902 - val_loss: 747.7137\n",
      "Epoch 19/35\n",
      "53817/53817 [==============================] - 107s - loss: 958.5745 - val_loss: 1126.3153\n",
      "Epoch 20/35\n",
      "53817/53817 [==============================] - 108s - loss: 953.5367 - val_loss: 762.3031\n",
      "Epoch 21/35\n",
      "53817/53817 [==============================] - 106s - loss: 935.4873 - val_loss: 999.5213\n",
      "Epoch 22/35\n",
      "53817/53817 [==============================] - 105s - loss: 920.0312 - val_loss: 721.3861\n",
      "Epoch 23/35\n",
      "53817/53817 [==============================] - 104s - loss: 918.6266 - val_loss: 763.5100\n",
      "Epoch 24/35\n",
      "53817/53817 [==============================] - 105s - loss: 908.5171 - val_loss: 713.9174\n",
      "Epoch 25/35\n",
      "53817/53817 [==============================] - 104s - loss: 900.0862 - val_loss: 837.5072\n",
      "Epoch 26/35\n",
      "53817/53817 [==============================] - 104s - loss: 889.1065 - val_loss: 688.1433\n",
      "Epoch 27/35\n",
      "53817/53817 [==============================] - 105s - loss: 892.6824 - val_loss: 770.2082\n",
      "Epoch 28/35\n",
      "53817/53817 [==============================] - 105s - loss: 882.1656 - val_loss: 726.8864\n",
      "Epoch 29/35\n",
      "53817/53817 [==============================] - 104s - loss: 880.4216 - val_loss: 680.0496\n",
      "Epoch 30/35\n",
      "53817/53817 [==============================] - 106s - loss: 873.0544 - val_loss: 717.0640\n",
      "Epoch 31/35\n",
      "53817/53817 [==============================] - 104s - loss: 865.6911 - val_loss: 689.4541\n",
      "Epoch 32/35\n",
      "53817/53817 [==============================] - 105s - loss: 871.8002 - val_loss: 675.5850\n",
      "Epoch 33/35\n",
      "53817/53817 [==============================] - 106s - loss: 864.2799 - val_loss: 684.6305\n",
      "Epoch 34/35\n",
      "53817/53817 [==============================] - 106s - loss: 847.1374 - val_loss: 651.5566\n",
      "Epoch 35/35\n",
      "53817/53817 [==============================] - 107s - loss: 849.8304 - val_loss: 718.3556\n",
      "Percent of variability explained by model: 67.8468598329\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF10_rep2_ear',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31403 samples, validate on 7851 samples\n",
      "Epoch 1/35\n",
      "31403/31403 [==============================] - 63s - loss: 1571.1728 - val_loss: 1351.4851\n",
      "Epoch 2/35\n",
      "31403/31403 [==============================] - 63s - loss: 1196.3053 - val_loss: 1037.7606\n",
      "Epoch 3/35\n",
      "31403/31403 [==============================] - 63s - loss: 1016.0788 - val_loss: 942.9680\n",
      "Epoch 4/35\n",
      "31403/31403 [==============================] - 63s - loss: 960.3343 - val_loss: 886.9431\n",
      "Epoch 5/35\n",
      "31403/31403 [==============================] - 62s - loss: 910.0408 - val_loss: 816.6931\n",
      "Epoch 6/35\n",
      "31403/31403 [==============================] - 62s - loss: 824.9694 - val_loss: 733.0281\n",
      "Epoch 7/35\n",
      "31403/31403 [==============================] - 62s - loss: 772.4587 - val_loss: 670.1912\n",
      "Epoch 8/35\n",
      "31403/31403 [==============================] - 62s - loss: 722.3292 - val_loss: 1066.9985\n",
      "Epoch 9/35\n",
      "31403/31403 [==============================] - 62s - loss: 696.5746 - val_loss: 757.6237\n",
      "Epoch 10/35\n",
      "31403/31403 [==============================] - 62s - loss: 663.6295 - val_loss: 615.5785\n",
      "Epoch 11/35\n",
      "31403/31403 [==============================] - 62s - loss: 647.9178 - val_loss: 761.9931\n",
      "Epoch 12/35\n",
      "31403/31403 [==============================] - 62s - loss: 625.0294 - val_loss: 736.0144\n",
      "Epoch 13/35\n",
      "31403/31403 [==============================] - 62s - loss: 607.5215 - val_loss: 653.8895\n",
      "Epoch 14/35\n",
      "31403/31403 [==============================] - 62s - loss: 584.9469 - val_loss: 654.1202\n",
      "Epoch 15/35\n",
      "31403/31403 [==============================] - 62s - loss: 562.0689 - val_loss: 537.7629\n",
      "Epoch 16/35\n",
      "31403/31403 [==============================] - 61s - loss: 553.4536 - val_loss: 566.3934\n",
      "Epoch 17/35\n",
      "31403/31403 [==============================] - 61s - loss: 535.9065 - val_loss: 531.9602\n",
      "Epoch 18/35\n",
      "31403/31403 [==============================] - 61s - loss: 534.4462 - val_loss: 544.7651\n",
      "Epoch 19/35\n",
      "31403/31403 [==============================] - 61s - loss: 520.9113 - val_loss: 572.7887\n",
      "Epoch 20/35\n",
      "31403/31403 [==============================] - 62s - loss: 512.0969 - val_loss: 493.9874\n",
      "Epoch 21/35\n",
      "31403/31403 [==============================] - 61s - loss: 499.3621 - val_loss: 463.6151\n",
      "Epoch 22/35\n",
      "31403/31403 [==============================] - 61s - loss: 500.6590 - val_loss: 476.1600\n",
      "Epoch 23/35\n",
      "31403/31403 [==============================] - 61s - loss: 490.8636 - val_loss: 456.1391\n",
      "Epoch 24/35\n",
      "31403/31403 [==============================] - 61s - loss: 482.2543 - val_loss: 522.2680\n",
      "Epoch 25/35\n",
      "31403/31403 [==============================] - 62s - loss: 478.4660 - val_loss: 534.0453\n",
      "Epoch 26/35\n",
      "31403/31403 [==============================] - 62s - loss: 471.5043 - val_loss: 455.2348\n",
      "Epoch 27/35\n",
      "31403/31403 [==============================] - 62s - loss: 472.7790 - val_loss: 548.4255\n",
      "Epoch 28/35\n",
      "31403/31403 [==============================] - 62s - loss: 464.2687 - val_loss: 444.4443\n",
      "Epoch 29/35\n",
      "31403/31403 [==============================] - 61s - loss: 464.8370 - val_loss: 490.7210\n",
      "Epoch 30/35\n",
      "31403/31403 [==============================] - 61s - loss: 461.7160 - val_loss: 435.3085\n",
      "Epoch 31/35\n",
      "31403/31403 [==============================] - 62s - loss: 459.0585 - val_loss: 433.1171\n",
      "Epoch 32/35\n",
      "31403/31403 [==============================] - 62s - loss: 458.5646 - val_loss: 479.2010\n",
      "Epoch 33/35\n",
      "31403/31403 [==============================] - 62s - loss: 444.8748 - val_loss: 450.6478\n",
      "Epoch 34/35\n",
      "31403/31403 [==============================] - 63s - loss: 442.6540 - val_loss: 453.4857\n",
      "Epoch 35/35\n",
      "31403/31403 [==============================] - 62s - loss: 448.3169 - val_loss: 435.4559\n",
      "Percent of variability explained by model: 68.895173477\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF10_rep1_tassel',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48745 samples, validate on 12187 samples\n",
      "Epoch 1/35\n",
      "48745/48745 [==============================] - 99s - loss: 2912.2922 - val_loss: 2496.6601\n",
      "Epoch 2/35\n",
      "48745/48745 [==============================] - 97s - loss: 2320.6934 - val_loss: 2059.1017\n",
      "Epoch 3/35\n",
      "48745/48745 [==============================] - 97s - loss: 2077.2247 - val_loss: 1963.1966\n",
      "Epoch 4/35\n",
      "48745/48745 [==============================] - 99s - loss: 1984.6060 - val_loss: 1876.5615\n",
      "Epoch 5/35\n",
      "48745/48745 [==============================] - 97s - loss: 1842.2177 - val_loss: 1759.1762\n",
      "Epoch 6/35\n",
      "48745/48745 [==============================] - 94s - loss: 1674.5266 - val_loss: 1510.3170\n",
      "Epoch 7/35\n",
      "48745/48745 [==============================] - 94s - loss: 1564.8009 - val_loss: 1660.5494\n",
      "Epoch 8/35\n",
      "48745/48745 [==============================] - 94s - loss: 1506.3860 - val_loss: 1413.6896\n",
      "Epoch 9/35\n",
      "48745/48745 [==============================] - 93s - loss: 1442.3534 - val_loss: 1337.6433\n",
      "Epoch 10/35\n",
      "48745/48745 [==============================] - 93s - loss: 1404.6619 - val_loss: 1291.9787\n",
      "Epoch 11/35\n",
      "48745/48745 [==============================] - 94s - loss: 1349.7194 - val_loss: 1416.7899\n",
      "Epoch 12/35\n",
      "48745/48745 [==============================] - 94s - loss: 1341.2044 - val_loss: 1226.6917\n",
      "Epoch 13/35\n",
      "48745/48745 [==============================] - 94s - loss: 1283.1535 - val_loss: 1164.9581\n",
      "Epoch 14/35\n",
      "48745/48745 [==============================] - 94s - loss: 1237.7734 - val_loss: 1209.1388\n",
      "Epoch 15/35\n",
      "48745/48745 [==============================] - 94s - loss: 1220.3711 - val_loss: 1250.4805\n",
      "Epoch 16/35\n",
      "48745/48745 [==============================] - 94s - loss: 1188.2384 - val_loss: 1114.6012\n",
      "Epoch 17/35\n",
      "48745/48745 [==============================] - 94s - loss: 1175.3719 - val_loss: 1066.0846\n",
      "Epoch 18/35\n",
      "48745/48745 [==============================] - 94s - loss: 1148.1204 - val_loss: 1072.0996\n",
      "Epoch 19/35\n",
      "48745/48745 [==============================] - 94s - loss: 1116.8075 - val_loss: 1030.3942\n",
      "Epoch 20/35\n",
      "48745/48745 [==============================] - 96s - loss: 1119.6857 - val_loss: 1066.2099\n",
      "Epoch 21/35\n",
      "48745/48745 [==============================] - 96s - loss: 1115.2190 - val_loss: 1087.2071\n",
      "Epoch 22/35\n",
      "48745/48745 [==============================] - 96s - loss: 1087.2154 - val_loss: 1009.0873\n",
      "Epoch 23/35\n",
      "48745/48745 [==============================] - 97s - loss: 1093.3352 - val_loss: 1132.4259\n",
      "Epoch 24/35\n",
      "48745/48745 [==============================] - 96s - loss: 1083.4726 - val_loss: 1097.8092\n",
      "Epoch 25/35\n",
      "48745/48745 [==============================] - 97s - loss: 1075.3678 - val_loss: 1064.3904\n",
      "Epoch 26/35\n",
      "48745/48745 [==============================] - 97s - loss: 1052.8227 - val_loss: 1074.2313\n",
      "Epoch 27/35\n",
      "48745/48745 [==============================] - 93s - loss: 1058.0004 - val_loss: 1000.6560\n",
      "Epoch 28/35\n",
      "48745/48745 [==============================] - 95s - loss: 1030.5848 - val_loss: 977.2433\n",
      "Epoch 29/35\n",
      "48745/48745 [==============================] - 97s - loss: 1030.7933 - val_loss: 981.8221\n",
      "Epoch 30/35\n",
      "48745/48745 [==============================] - 97s - loss: 1026.8462 - val_loss: 987.1618\n",
      "Epoch 31/35\n",
      "48745/48745 [==============================] - 96s - loss: 1009.8915 - val_loss: 957.4260\n",
      "Epoch 32/35\n",
      "48745/48745 [==============================] - 97s - loss: 996.6200 - val_loss: 988.3522\n",
      "Epoch 33/35\n",
      "48745/48745 [==============================] - 94s - loss: 1007.0529 - val_loss: 1091.2293\n",
      "Epoch 34/35\n",
      "48745/48745 [==============================] - 96s - loss: 993.3373 - val_loss: 1024.1513\n",
      "Epoch 35/35\n",
      "48745/48745 [==============================] - 97s - loss: 997.0950 - val_loss: 952.6439\n",
      "Percent of variability explained by model: 63.8616784321\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF10_rep2_tassel',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36776 samples, validate on 9195 samples\n",
      "Epoch 1/35\n",
      "36776/36776 [==============================] - 73s - loss: 2127.7159 - val_loss: 1398.1829\n",
      "Epoch 2/35\n",
      "36776/36776 [==============================] - 70s - loss: 1752.4591 - val_loss: 1306.4759\n",
      "Epoch 3/35\n",
      "36776/36776 [==============================] - 70s - loss: 1740.4532 - val_loss: 1299.6157\n",
      "Epoch 4/35\n",
      "36776/36776 [==============================] - 69s - loss: 1711.7987 - val_loss: 1246.7413\n",
      "Epoch 5/35\n",
      "36776/36776 [==============================] - 68s - loss: 1670.8104 - val_loss: 1206.6119\n",
      "Epoch 6/35\n",
      "36776/36776 [==============================] - 68s - loss: 1615.2978 - val_loss: 1275.9449\n",
      "Epoch 7/35\n",
      "36776/36776 [==============================] - 68s - loss: 1516.6498 - val_loss: 954.5047\n",
      "Epoch 8/35\n",
      "36776/36776 [==============================] - 68s - loss: 1433.8557 - val_loss: 1005.2452\n",
      "Epoch 9/35\n",
      "36776/36776 [==============================] - 68s - loss: 1382.1148 - val_loss: 1381.7839\n",
      "Epoch 10/35\n",
      "36776/36776 [==============================] - 68s - loss: 1339.7526 - val_loss: 810.4373\n",
      "Epoch 11/35\n",
      "36776/36776 [==============================] - 68s - loss: 1309.0340 - val_loss: 758.4144\n",
      "Epoch 12/35\n",
      "36776/36776 [==============================] - 68s - loss: 1265.7970 - val_loss: 778.1698\n",
      "Epoch 13/35\n",
      "36776/36776 [==============================] - 68s - loss: 1244.8440 - val_loss: 717.1294\n",
      "Epoch 14/35\n",
      "36776/36776 [==============================] - 68s - loss: 1218.7533 - val_loss: 758.9011\n",
      "Epoch 15/35\n",
      "36776/36776 [==============================] - 68s - loss: 1205.1691 - val_loss: 694.1806\n",
      "Epoch 16/35\n",
      "36776/36776 [==============================] - 68s - loss: 1189.4177 - val_loss: 710.1930\n",
      "Epoch 17/35\n",
      "36776/36776 [==============================] - 68s - loss: 1180.0052 - val_loss: 662.9750\n",
      "Epoch 18/35\n",
      "36776/36776 [==============================] - 68s - loss: 1169.0842 - val_loss: 646.1173\n",
      "Epoch 19/35\n",
      "36776/36776 [==============================] - 68s - loss: 1153.7126 - val_loss: 641.0596\n",
      "Epoch 20/35\n",
      "36776/36776 [==============================] - 68s - loss: 1138.3302 - val_loss: 707.3716\n",
      "Epoch 21/35\n",
      "36776/36776 [==============================] - 68s - loss: 1117.4906 - val_loss: 620.7583\n",
      "Epoch 22/35\n",
      "36776/36776 [==============================] - 68s - loss: 1113.2114 - val_loss: 1048.4001\n",
      "Epoch 23/35\n",
      "36776/36776 [==============================] - 68s - loss: 1101.6340 - val_loss: 619.3405\n",
      "Epoch 24/35\n",
      "36776/36776 [==============================] - 68s - loss: 1097.6661 - val_loss: 597.2583\n",
      "Epoch 25/35\n",
      "36776/36776 [==============================] - 68s - loss: 1080.0793 - val_loss: 586.4690\n",
      "Epoch 26/35\n",
      "36776/36776 [==============================] - 68s - loss: 1071.9089 - val_loss: 615.8962\n",
      "Epoch 27/35\n",
      "36776/36776 [==============================] - 68s - loss: 1056.1401 - val_loss: 608.3375\n",
      "Epoch 28/35\n",
      "36776/36776 [==============================] - 68s - loss: 1037.3000 - val_loss: 561.6544\n",
      "Epoch 29/35\n",
      "36776/36776 [==============================] - 68s - loss: 1041.3492 - val_loss: 571.3624\n",
      "Epoch 30/35\n",
      "36776/36776 [==============================] - 68s - loss: 1022.5433 - val_loss: 579.4958\n",
      "Epoch 31/35\n",
      "36776/36776 [==============================] - 68s - loss: 1031.0422 - val_loss: 571.7467\n",
      "Epoch 32/35\n",
      "36776/36776 [==============================] - 68s - loss: 1025.5849 - val_loss: 554.6889\n",
      "Epoch 33/35\n",
      "36776/36776 [==============================] - 68s - loss: 1011.8382 - val_loss: 550.5131\n",
      "Epoch 34/35\n",
      "36776/36776 [==============================] - 68s - loss: 1005.2707 - val_loss: 561.1088\n",
      "Epoch 35/35\n",
      "36776/36776 [==============================] - 68s - loss: 1003.2419 - val_loss: 539.1393\n",
      "Percent of variability explained by model: 58.1738098723\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF4',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19473 samples, validate on 4869 samples\n",
      "Epoch 1/35\n",
      "19473/19473 [==============================] - 37s - loss: 1679.4559 - val_loss: 1132.3133\n",
      "Epoch 2/35\n",
      "19473/19473 [==============================] - 36s - loss: 1383.0800 - val_loss: 931.4516\n",
      "Epoch 3/35\n",
      "19473/19473 [==============================] - 36s - loss: 1301.4132 - val_loss: 913.7392\n",
      "Epoch 4/35\n",
      "19473/19473 [==============================] - 36s - loss: 1290.4728 - val_loss: 912.5556\n",
      "Epoch 5/35\n",
      "19473/19473 [==============================] - 36s - loss: 1277.8503 - val_loss: 865.1717\n",
      "Epoch 6/35\n",
      "19473/19473 [==============================] - 36s - loss: 1282.8576 - val_loss: 891.8963\n",
      "Epoch 7/35\n",
      "19473/19473 [==============================] - 36s - loss: 1272.3135 - val_loss: 871.9457\n",
      "Epoch 8/35\n",
      "19473/19473 [==============================] - 36s - loss: 1244.8601 - val_loss: 861.7561\n",
      "Epoch 9/35\n",
      "19473/19473 [==============================] - 36s - loss: 1216.9461 - val_loss: 803.5947\n",
      "Epoch 10/35\n",
      "19473/19473 [==============================] - 36s - loss: 1181.0971 - val_loss: 766.5046\n",
      "Epoch 11/35\n",
      "19473/19473 [==============================] - 36s - loss: 1120.1579 - val_loss: 873.6869\n",
      "Epoch 12/35\n",
      "19473/19473 [==============================] - 36s - loss: 1088.7480 - val_loss: 651.1982\n",
      "Epoch 13/35\n",
      "19473/19473 [==============================] - 36s - loss: 1043.4052 - val_loss: 752.4677\n",
      "Epoch 14/35\n",
      "19473/19473 [==============================] - 36s - loss: 1029.7515 - val_loss: 593.4123\n",
      "Epoch 15/35\n",
      "19473/19473 [==============================] - 36s - loss: 1006.2415 - val_loss: 537.1680\n",
      "Epoch 16/35\n",
      "19473/19473 [==============================] - 36s - loss: 989.9344 - val_loss: 774.3669\n",
      "Epoch 17/35\n",
      "19473/19473 [==============================] - 36s - loss: 984.4070 - val_loss: 597.0228\n",
      "Epoch 18/35\n",
      "19473/19473 [==============================] - 36s - loss: 952.9973 - val_loss: 522.0613\n",
      "Epoch 19/35\n",
      "19473/19473 [==============================] - 36s - loss: 944.6987 - val_loss: 522.8866\n",
      "Epoch 20/35\n",
      "19473/19473 [==============================] - 36s - loss: 922.0551 - val_loss: 548.2800\n",
      "Epoch 21/35\n",
      "19473/19473 [==============================] - 36s - loss: 916.5126 - val_loss: 522.4950\n",
      "Epoch 22/35\n",
      "19473/19473 [==============================] - 36s - loss: 903.4039 - val_loss: 620.2227\n",
      "Epoch 23/35\n",
      "19473/19473 [==============================] - 36s - loss: 895.7719 - val_loss: 550.5333\n",
      "Epoch 24/35\n",
      "19473/19473 [==============================] - 36s - loss: 896.0578 - val_loss: 530.9172\n",
      "Epoch 25/35\n",
      "19473/19473 [==============================] - 36s - loss: 880.7782 - val_loss: 461.1192\n",
      "Epoch 26/35\n",
      "19473/19473 [==============================] - 36s - loss: 874.0875 - val_loss: 469.8853\n",
      "Epoch 27/35\n",
      "19473/19473 [==============================] - 36s - loss: 860.5315 - val_loss: 468.0470\n",
      "Epoch 28/35\n",
      "19473/19473 [==============================] - 36s - loss: 846.8996 - val_loss: 442.2312\n",
      "Epoch 29/35\n",
      "19473/19473 [==============================] - 36s - loss: 845.1063 - val_loss: 494.2035\n",
      "Epoch 30/35\n",
      "19473/19473 [==============================] - 36s - loss: 833.3395 - val_loss: 472.8117\n",
      "Epoch 31/35\n",
      "19473/19473 [==============================] - 36s - loss: 827.7247 - val_loss: 437.4095\n",
      "Epoch 32/35\n",
      "19473/19473 [==============================] - 36s - loss: 813.2496 - val_loss: 418.6183\n",
      "Epoch 33/35\n",
      "19473/19473 [==============================] - 36s - loss: 807.6633 - val_loss: 504.5649\n",
      "Epoch 34/35\n",
      "19473/19473 [==============================] - 36s - loss: 802.3556 - val_loss: 416.9182\n",
      "Epoch 35/35\n",
      "19473/19473 [==============================] - 36s - loss: 795.5772 - val_loss: 411.3676\n",
      "Percent of variability explained by model: 52.8280910487\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF4_rep2',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36190 samples, validate on 9048 samples\n",
      "Epoch 1/35\n",
      "36190/36190 [==============================] - 72s - loss: 3367.6591 - val_loss: 3307.8301\n",
      "Epoch 2/35\n",
      "36190/36190 [==============================] - 71s - loss: 2849.5019 - val_loss: 3014.9194\n",
      "Epoch 3/35\n",
      "36190/36190 [==============================] - 71s - loss: 2779.7190 - val_loss: 3007.9033\n",
      "Epoch 4/35\n",
      "36190/36190 [==============================] - 69s - loss: 2758.3360 - val_loss: 3007.5720\n",
      "Epoch 5/35\n",
      "36190/36190 [==============================] - 67s - loss: 2726.9250 - val_loss: 2967.4282\n",
      "Epoch 6/35\n",
      "36190/36190 [==============================] - 67s - loss: 2714.6290 - val_loss: 2896.4135\n",
      "Epoch 7/35\n",
      "36190/36190 [==============================] - 67s - loss: 2638.8910 - val_loss: 2822.2334\n",
      "Epoch 8/35\n",
      "36190/36190 [==============================] - 67s - loss: 2562.6571 - val_loss: 2736.8975\n",
      "Epoch 9/35\n",
      "36190/36190 [==============================] - 67s - loss: 2480.9359 - val_loss: 2673.9643\n",
      "Epoch 10/35\n",
      "36190/36190 [==============================] - 69s - loss: 2416.7002 - val_loss: 2586.1669\n",
      "Epoch 11/35\n",
      "36190/36190 [==============================] - 69s - loss: 2353.3997 - val_loss: 2438.5993\n",
      "Epoch 12/35\n",
      "36190/36190 [==============================] - 67s - loss: 2291.7351 - val_loss: 2813.4242\n",
      "Epoch 13/35\n",
      "36190/36190 [==============================] - 68s - loss: 2243.8905 - val_loss: 2330.0041\n",
      "Epoch 14/35\n",
      "36190/36190 [==============================] - 68s - loss: 2184.4594 - val_loss: 2391.5287\n",
      "Epoch 15/35\n",
      "36190/36190 [==============================] - 68s - loss: 2167.9827 - val_loss: 2257.3522\n",
      "Epoch 16/35\n",
      "36190/36190 [==============================] - 68s - loss: 2120.6718 - val_loss: 2229.8465\n",
      "Epoch 17/35\n",
      "36190/36190 [==============================] - 67s - loss: 2099.1641 - val_loss: 2245.2091\n",
      "Epoch 18/35\n",
      "36190/36190 [==============================] - 68s - loss: 2065.4634 - val_loss: 2204.9971\n",
      "Epoch 19/35\n",
      "36190/36190 [==============================] - 68s - loss: 2042.2773 - val_loss: 2175.3797\n",
      "Epoch 20/35\n",
      "36190/36190 [==============================] - 69s - loss: 2014.7688 - val_loss: 2234.6879\n",
      "Epoch 21/35\n",
      "36190/36190 [==============================] - 70s - loss: 1998.4618 - val_loss: 2118.2103\n",
      "Epoch 22/35\n",
      "36190/36190 [==============================] - 72s - loss: 1962.6600 - val_loss: 2274.4610\n",
      "Epoch 23/35\n",
      "36190/36190 [==============================] - 71s - loss: 1934.9538 - val_loss: 2088.0149\n",
      "Epoch 24/35\n",
      "36190/36190 [==============================] - 71s - loss: 1925.7744 - val_loss: 2150.9025\n",
      "Epoch 25/35\n",
      "36190/36190 [==============================] - 71s - loss: 1889.1768 - val_loss: 2088.1324\n",
      "Epoch 26/35\n",
      "36190/36190 [==============================] - 71s - loss: 1881.2483 - val_loss: 2028.0651\n",
      "Epoch 27/35\n",
      "36190/36190 [==============================] - 72s - loss: 1864.5443 - val_loss: 2026.0519\n",
      "Epoch 28/35\n",
      "36190/36190 [==============================] - 72s - loss: 1841.3566 - val_loss: 2015.3970\n",
      "Epoch 29/35\n",
      "36190/36190 [==============================] - 72s - loss: 1846.9823 - val_loss: 2001.9727\n",
      "Epoch 30/35\n",
      "36190/36190 [==============================] - 72s - loss: 1838.7619 - val_loss: 1986.5135\n",
      "Epoch 31/35\n",
      "36190/36190 [==============================] - 71s - loss: 1817.7643 - val_loss: 2006.2423\n",
      "Epoch 32/35\n",
      "36190/36190 [==============================] - 71s - loss: 1819.5251 - val_loss: 2011.5963\n",
      "Epoch 33/35\n",
      "36190/36190 [==============================] - 72s - loss: 1797.4400 - val_loss: 1981.8943\n",
      "Epoch 34/35\n",
      "36190/36190 [==============================] - 71s - loss: 1801.6066 - val_loss: 2029.0007\n",
      "Epoch 35/35\n",
      "36190/36190 [==============================] - 71s - loss: 1784.2935 - val_loss: 2329.1623\n",
      "Percent of variability explained by model: 30.5283859963\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF4_rep3',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42963 samples, validate on 10741 samples\n",
      "Epoch 1/35\n",
      "42963/42963 [==============================] - 82s - loss: 2364.4806 - val_loss: 2013.0646\n",
      "Epoch 2/35\n",
      "42963/42963 [==============================] - 81s - loss: 1734.5396 - val_loss: 1463.8579\n",
      "Epoch 3/35\n",
      "42963/42963 [==============================] - 81s - loss: 1371.7000 - val_loss: 1130.8974\n",
      "Epoch 4/35\n",
      "42963/42963 [==============================] - 82s - loss: 1180.5277 - val_loss: 1069.9976\n",
      "Epoch 5/35\n",
      "42963/42963 [==============================] - 81s - loss: 1087.6656 - val_loss: 932.7324\n",
      "Epoch 6/35\n",
      "42963/42963 [==============================] - 80s - loss: 1046.5845 - val_loss: 866.3762\n",
      "Epoch 7/35\n",
      "42963/42963 [==============================] - 80s - loss: 1008.9000 - val_loss: 827.6527\n",
      "Epoch 8/35\n",
      "42963/42963 [==============================] - 79s - loss: 970.7698 - val_loss: 844.0327\n",
      "Epoch 9/35\n",
      "42963/42963 [==============================] - 79s - loss: 946.1754 - val_loss: 806.4745\n",
      "Epoch 10/35\n",
      "42963/42963 [==============================] - 80s - loss: 905.3566 - val_loss: 700.8230\n",
      "Epoch 11/35\n",
      "42963/42963 [==============================] - 81s - loss: 888.4429 - val_loss: 792.8951\n",
      "Epoch 12/35\n",
      "42963/42963 [==============================] - 82s - loss: 855.2450 - val_loss: 650.2038\n",
      "Epoch 13/35\n",
      "42963/42963 [==============================] - 82s - loss: 844.4491 - val_loss: 651.8952\n",
      "Epoch 14/35\n",
      "42963/42963 [==============================] - 82s - loss: 821.2733 - val_loss: 611.5214\n",
      "Epoch 15/35\n",
      "42963/42963 [==============================] - 82s - loss: 814.4304 - val_loss: 597.7862\n",
      "Epoch 16/35\n",
      "42963/42963 [==============================] - 82s - loss: 809.0766 - val_loss: 635.1612\n",
      "Epoch 17/35\n",
      "42963/42963 [==============================] - 82s - loss: 794.8655 - val_loss: 584.2861\n",
      "Epoch 18/35\n",
      "42963/42963 [==============================] - 82s - loss: 778.2633 - val_loss: 589.0683\n",
      "Epoch 19/35\n",
      "42963/42963 [==============================] - 82s - loss: 780.9996 - val_loss: 571.9919\n",
      "Epoch 20/35\n",
      "42963/42963 [==============================] - 82s - loss: 743.9281 - val_loss: 577.8172\n",
      "Epoch 21/35\n",
      "42963/42963 [==============================] - 82s - loss: 745.0407 - val_loss: 615.2937\n",
      "Epoch 22/35\n",
      "42963/42963 [==============================] - 82s - loss: 733.4389 - val_loss: 574.7604\n",
      "Epoch 23/35\n",
      "42963/42963 [==============================] - 82s - loss: 737.5555 - val_loss: 571.2478\n",
      "Epoch 24/35\n",
      "42963/42963 [==============================] - 82s - loss: 729.0695 - val_loss: 563.6074\n",
      "Epoch 25/35\n",
      "42963/42963 [==============================] - 82s - loss: 724.6396 - val_loss: 560.4709\n",
      "Epoch 26/35\n",
      "42963/42963 [==============================] - 82s - loss: 732.8482 - val_loss: 554.8340\n",
      "Epoch 27/35\n",
      "42963/42963 [==============================] - 82s - loss: 712.6953 - val_loss: 546.8854\n",
      "Epoch 28/35\n",
      "42963/42963 [==============================] - 82s - loss: 714.6152 - val_loss: 540.0943\n",
      "Epoch 29/35\n",
      "42963/42963 [==============================] - 82s - loss: 706.9493 - val_loss: 557.3089\n",
      "Epoch 30/35\n",
      "42963/42963 [==============================] - 82s - loss: 696.9899 - val_loss: 545.6832\n",
      "Epoch 31/35\n",
      "42963/42963 [==============================] - 82s - loss: 706.4488 - val_loss: 587.4052\n",
      "Epoch 32/35\n",
      "42963/42963 [==============================] - 82s - loss: 693.2072 - val_loss: 542.8237\n",
      "Epoch 33/35\n",
      "42963/42963 [==============================] - 82s - loss: 685.5955 - val_loss: 537.8628\n",
      "Epoch 34/35\n",
      "42963/42963 [==============================] - 81s - loss: 688.4545 - val_loss: 561.4629\n",
      "Epoch 35/35\n",
      "42963/42963 [==============================] - 81s - loss: 696.0120 - val_loss: 542.0661\n",
      "Percent of variability explained by model: 74.8382009504\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF10',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23500 samples, validate on 5876 samples\n",
      "Epoch 1/35\n",
      "23500/23500 [==============================] - 45s - loss: 3260.6062 - val_loss: 2878.4745\n",
      "Epoch 2/35\n",
      "23500/23500 [==============================] - 45s - loss: 2730.0433 - val_loss: 2466.1536\n",
      "Epoch 3/35\n",
      "23500/23500 [==============================] - 45s - loss: 2282.1756 - val_loss: 1842.5053\n",
      "Epoch 4/35\n",
      "23500/23500 [==============================] - 45s - loss: 1844.0153 - val_loss: 1406.7536\n",
      "Epoch 5/35\n",
      "23500/23500 [==============================] - 45s - loss: 1541.0483 - val_loss: 1129.5317\n",
      "Epoch 6/35\n",
      "23500/23500 [==============================] - 45s - loss: 1379.6999 - val_loss: 1061.9651\n",
      "Epoch 7/35\n",
      "23500/23500 [==============================] - 45s - loss: 1297.5732 - val_loss: 1030.1212\n",
      "Epoch 8/35\n",
      "23500/23500 [==============================] - 45s - loss: 1242.7591 - val_loss: 913.2825\n",
      "Epoch 9/35\n",
      "23500/23500 [==============================] - 44s - loss: 1189.4518 - val_loss: 908.1424\n",
      "Epoch 10/35\n",
      "23500/23500 [==============================] - 44s - loss: 1164.2450 - val_loss: 919.6730\n",
      "Epoch 11/35\n",
      "23500/23500 [==============================] - 44s - loss: 1124.0101 - val_loss: 843.7477\n",
      "Epoch 12/35\n",
      "23500/23500 [==============================] - 44s - loss: 1096.5662 - val_loss: 810.6230\n",
      "Epoch 13/35\n",
      "23500/23500 [==============================] - 44s - loss: 1077.7338 - val_loss: 918.9437\n",
      "Epoch 14/35\n",
      "23500/23500 [==============================] - 44s - loss: 1050.4621 - val_loss: 809.8206\n",
      "Epoch 15/35\n",
      "23500/23500 [==============================] - 44s - loss: 1025.0704 - val_loss: 791.6479\n",
      "Epoch 16/35\n",
      "23500/23500 [==============================] - 44s - loss: 1006.1381 - val_loss: 742.1155\n",
      "Epoch 17/35\n",
      "23500/23500 [==============================] - 44s - loss: 991.7388 - val_loss: 722.9475\n",
      "Epoch 18/35\n",
      "23500/23500 [==============================] - 44s - loss: 967.7184 - val_loss: 702.3437\n",
      "Epoch 19/35\n",
      "23500/23500 [==============================] - 44s - loss: 955.5380 - val_loss: 690.7325\n",
      "Epoch 20/35\n",
      "23500/23500 [==============================] - 44s - loss: 928.0433 - val_loss: 689.0414\n",
      "Epoch 21/35\n",
      "23500/23500 [==============================] - 44s - loss: 914.2803 - val_loss: 653.0092\n",
      "Epoch 22/35\n",
      "23500/23500 [==============================] - 44s - loss: 893.9404 - val_loss: 640.3976\n",
      "Epoch 23/35\n",
      "23500/23500 [==============================] - 44s - loss: 885.3753 - val_loss: 670.1932\n",
      "Epoch 24/35\n",
      "23500/23500 [==============================] - 44s - loss: 874.1493 - val_loss: 692.9627\n",
      "Epoch 25/35\n",
      "23500/23500 [==============================] - 44s - loss: 873.2505 - val_loss: 718.7199\n",
      "Epoch 26/35\n",
      "23500/23500 [==============================] - 44s - loss: 847.1545 - val_loss: 596.9739\n",
      "Epoch 27/35\n",
      "23500/23500 [==============================] - 44s - loss: 842.9350 - val_loss: 588.9807\n",
      "Epoch 28/35\n",
      "23500/23500 [==============================] - 44s - loss: 822.8153 - val_loss: 573.3144\n",
      "Epoch 29/35\n",
      "23500/23500 [==============================] - 44s - loss: 815.3163 - val_loss: 598.0026\n",
      "Epoch 30/35\n",
      "23500/23500 [==============================] - 44s - loss: 801.5074 - val_loss: 715.1495\n",
      "Epoch 31/35\n",
      "23500/23500 [==============================] - 44s - loss: 781.1472 - val_loss: 570.6527\n",
      "Epoch 32/35\n",
      "23500/23500 [==============================] - 44s - loss: 789.7735 - val_loss: 657.8060\n",
      "Epoch 33/35\n",
      "23500/23500 [==============================] - 44s - loss: 776.8453 - val_loss: 554.1165\n",
      "Epoch 34/35\n",
      "23500/23500 [==============================] - 44s - loss: 767.8657 - val_loss: 585.7433\n",
      "Epoch 35/35\n",
      "23500/23500 [==============================] - 44s - loss: 785.0858 - val_loss: 547.1795\n",
      "Percent of variability explained by model: 73.9722611215\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF13',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53941 samples, validate on 13486 samples\n",
      "Epoch 1/35\n",
      "53941/53941 [==============================] - 104s - loss: 9463.3411 - val_loss: 11466.4891\n",
      "Epoch 2/35\n",
      "53941/53941 [==============================] - 103s - loss: 8558.3027 - val_loss: 11119.7556\n",
      "Epoch 3/35\n",
      "53941/53941 [==============================] - 103s - loss: 8424.2766 - val_loss: 10964.1154\n",
      "Epoch 4/35\n",
      "53941/53941 [==============================] - 103s - loss: 8299.6816 - val_loss: 10780.1169\n",
      "Epoch 5/35\n",
      "53941/53941 [==============================] - 103s - loss: 8038.9400 - val_loss: 10451.6659\n",
      "Epoch 6/35\n",
      "53941/53941 [==============================] - 103s - loss: 7760.9728 - val_loss: 10418.1807\n",
      "Epoch 7/35\n",
      "53941/53941 [==============================] - 102s - loss: 7485.4884 - val_loss: 10147.1557\n",
      "Epoch 8/35\n",
      "53941/53941 [==============================] - 102s - loss: 7339.0710 - val_loss: 10459.5790\n",
      "Epoch 9/35\n",
      "53941/53941 [==============================] - 103s - loss: 7212.2982 - val_loss: 9553.9941\n",
      "Epoch 10/35\n",
      "53941/53941 [==============================] - 102s - loss: 7093.0289 - val_loss: 9460.1840\n",
      "Epoch 11/35\n",
      "53941/53941 [==============================] - 102s - loss: 7017.3432 - val_loss: 9558.7582\n",
      "Epoch 12/35\n",
      "53941/53941 [==============================] - 102s - loss: 6941.5186 - val_loss: 9613.7603\n",
      "Epoch 13/35\n",
      "53941/53941 [==============================] - 102s - loss: 6837.4931 - val_loss: 9279.6793\n",
      "Epoch 14/35\n",
      "53941/53941 [==============================] - 102s - loss: 6738.4136 - val_loss: 9950.7435\n",
      "Epoch 15/35\n",
      "53941/53941 [==============================] - 103s - loss: 6691.7959 - val_loss: 9318.6941\n",
      "Epoch 16/35\n",
      "53941/53941 [==============================] - 102s - loss: 6608.1653 - val_loss: 9014.6074\n",
      "Epoch 17/35\n",
      "53941/53941 [==============================] - 102s - loss: 6522.9413 - val_loss: 9111.3076\n",
      "Epoch 18/35\n",
      "53941/53941 [==============================] - 102s - loss: 6492.4264 - val_loss: 8981.8711\n",
      "Epoch 19/35\n",
      "53941/53941 [==============================] - 102s - loss: 6419.5070 - val_loss: 9084.3887\n",
      "Epoch 20/35\n",
      "53941/53941 [==============================] - 102s - loss: 6355.4390 - val_loss: 9008.5528\n",
      "Epoch 21/35\n",
      "53941/53941 [==============================] - 103s - loss: 6334.8788 - val_loss: 8860.3204\n",
      "Epoch 22/35\n",
      "53941/53941 [==============================] - 102s - loss: 6258.1236 - val_loss: 8819.6715\n",
      "Epoch 23/35\n",
      "53941/53941 [==============================] - 102s - loss: 6213.1623 - val_loss: 9149.9568\n",
      "Epoch 24/35\n",
      "53941/53941 [==============================] - 102s - loss: 6177.2981 - val_loss: 8809.9202\n",
      "Epoch 25/35\n",
      "53941/53941 [==============================] - 102s - loss: 6123.7487 - val_loss: 8924.4985\n",
      "Epoch 26/35\n",
      "53941/53941 [==============================] - 102s - loss: 6130.9793 - val_loss: 8933.6373\n",
      "Epoch 27/35\n",
      "53941/53941 [==============================] - 102s - loss: 6065.7940 - val_loss: 8732.7928\n",
      "Epoch 28/35\n",
      "53941/53941 [==============================] - 102s - loss: 6060.7093 - val_loss: 8961.4107\n",
      "Epoch 29/35\n",
      "53941/53941 [==============================] - 102s - loss: 6043.2317 - val_loss: 8627.6769\n",
      "Epoch 30/35\n",
      "53941/53941 [==============================] - 102s - loss: 6008.1131 - val_loss: 8586.5488\n",
      "Epoch 31/35\n",
      "53941/53941 [==============================] - 102s - loss: 6001.5337 - val_loss: 8759.5860\n",
      "Epoch 32/35\n",
      "53941/53941 [==============================] - 102s - loss: 5986.1187 - val_loss: 8682.4249\n",
      "Epoch 33/35\n",
      "53941/53941 [==============================] - 102s - loss: 5933.0512 - val_loss: 8978.9609\n",
      "Epoch 34/35\n",
      "53941/53941 [==============================] - 102s - loss: 5903.0483 - val_loss: 8812.1026\n",
      "Epoch 35/35\n",
      "53941/53941 [==============================] - 102s - loss: 5942.3226 - val_loss: 8768.1463\n",
      "Percent of variability explained by model: 34.3051068112\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF16',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9536 samples, validate on 2384 samples\n",
      "Epoch 1/35\n",
      "9536/9536 [==============================] - 19s - loss: 4551.3087 - val_loss: 3765.3687\n",
      "Epoch 2/35\n",
      "9536/9536 [==============================] - 18s - loss: 4264.2203 - val_loss: 3494.2492\n",
      "Epoch 3/35\n",
      "9536/9536 [==============================] - 18s - loss: 3976.0749 - val_loss: 3214.3347\n",
      "Epoch 4/35\n",
      "9536/9536 [==============================] - 18s - loss: 3750.8471 - val_loss: 3001.1992\n",
      "Epoch 5/35\n",
      "9536/9536 [==============================] - 18s - loss: 3623.5280 - val_loss: 2897.0141\n",
      "Epoch 6/35\n",
      "9536/9536 [==============================] - 18s - loss: 3576.6640 - val_loss: 2878.4662\n",
      "Epoch 7/35\n",
      "9536/9536 [==============================] - 18s - loss: 3555.4869 - val_loss: 2872.2026\n",
      "Epoch 8/35\n",
      "9536/9536 [==============================] - 18s - loss: 3559.6610 - val_loss: 2871.8774\n",
      "Epoch 9/35\n",
      "9536/9536 [==============================] - 18s - loss: 3544.3018 - val_loss: 2865.2712\n",
      "Epoch 10/35\n",
      "9536/9536 [==============================] - 18s - loss: 3540.9527 - val_loss: 2835.2304\n",
      "Epoch 11/35\n",
      "9536/9536 [==============================] - 18s - loss: 3522.4646 - val_loss: 2826.2941\n",
      "Epoch 12/35\n",
      "9536/9536 [==============================] - 18s - loss: 3537.5548 - val_loss: 2805.6569\n",
      "Epoch 13/35\n",
      "9536/9536 [==============================] - 18s - loss: 3467.2021 - val_loss: 2789.0331\n",
      "Epoch 14/35\n",
      "9536/9536 [==============================] - 18s - loss: 3482.4845 - val_loss: 2775.8624\n",
      "Epoch 15/35\n",
      "9536/9536 [==============================] - 18s - loss: 3466.3659 - val_loss: 2760.7009\n",
      "Epoch 16/35\n",
      "9536/9536 [==============================] - 18s - loss: 3441.9068 - val_loss: 2742.0459\n",
      "Epoch 17/35\n",
      "9536/9536 [==============================] - 18s - loss: 3427.8328 - val_loss: 2734.1413\n",
      "Epoch 18/35\n",
      "9536/9536 [==============================] - 18s - loss: 3417.9450 - val_loss: 2712.7061\n",
      "Epoch 19/35\n",
      "9536/9536 [==============================] - 18s - loss: 3387.6997 - val_loss: 2689.6157\n",
      "Epoch 20/35\n",
      "9536/9536 [==============================] - 18s - loss: 3350.0506 - val_loss: 2608.8226\n",
      "Epoch 21/35\n",
      "9536/9536 [==============================] - 18s - loss: 3296.8725 - val_loss: 2578.7761\n",
      "Epoch 22/35\n",
      "9536/9536 [==============================] - 18s - loss: 3225.0268 - val_loss: 2776.9235\n",
      "Epoch 23/35\n",
      "9536/9536 [==============================] - 18s - loss: 3200.8930 - val_loss: 2569.2358\n",
      "Epoch 24/35\n",
      "9536/9536 [==============================] - 18s - loss: 3165.5454 - val_loss: 2366.1596\n",
      "Epoch 25/35\n",
      "9536/9536 [==============================] - 18s - loss: 3126.6468 - val_loss: 2395.1955\n",
      "Epoch 26/35\n",
      "9536/9536 [==============================] - 18s - loss: 3084.9360 - val_loss: 2281.6529\n",
      "Epoch 27/35\n",
      "9536/9536 [==============================] - 18s - loss: 3039.4327 - val_loss: 2307.9124\n",
      "Epoch 28/35\n",
      "9536/9536 [==============================] - 18s - loss: 3042.0998 - val_loss: 2357.1558\n",
      "Epoch 29/35\n",
      "9536/9536 [==============================] - 18s - loss: 3016.1919 - val_loss: 2280.4380\n",
      "Epoch 30/35\n",
      "9536/9536 [==============================] - 18s - loss: 2995.7759 - val_loss: 2166.7773\n",
      "Epoch 31/35\n",
      "9536/9536 [==============================] - 18s - loss: 2940.8803 - val_loss: 2155.1909\n",
      "Epoch 32/35\n",
      "9536/9536 [==============================] - 18s - loss: 2927.0390 - val_loss: 2126.2424\n",
      "Epoch 33/35\n",
      "9536/9536 [==============================] - 18s - loss: 2906.0545 - val_loss: 2352.7121\n",
      "Epoch 34/35\n",
      "9536/9536 [==============================] - 18s - loss: 2858.5164 - val_loss: 2060.4132\n",
      "Epoch 35/35\n",
      "9536/9536 [==============================] - 18s - loss: 2836.1459 - val_loss: 2121.8501\n",
      "Percent of variability explained by model: 26.0710017312\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF18',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76796 samples, validate on 19200 samples\n",
      "Epoch 1/35\n",
      "76796/76796 [==============================] - 147s - loss: 11567.5999 - val_loss: 7797.8566\n",
      "Epoch 2/35\n",
      "76796/76796 [==============================] - 146s - loss: 10485.7554 - val_loss: 7440.0080\n",
      "Epoch 3/35\n",
      "76796/76796 [==============================] - 147s - loss: 10331.7848 - val_loss: 7394.0962\n",
      "Epoch 4/35\n",
      "76796/76796 [==============================] - 146s - loss: 10100.9652 - val_loss: 7577.4496\n",
      "Epoch 5/35\n",
      "76796/76796 [==============================] - 146s - loss: 9793.3170 - val_loss: 6915.0043\n",
      "Epoch 6/35\n",
      "76796/76796 [==============================] - 146s - loss: 9502.8091 - val_loss: 6240.1487\n",
      "Epoch 7/35\n",
      "76796/76796 [==============================] - 146s - loss: 9284.4717 - val_loss: 6883.5863\n",
      "Epoch 8/35\n",
      "76796/76796 [==============================] - 146s - loss: 9105.8293 - val_loss: 6142.5660\n",
      "Epoch 9/35\n",
      "76796/76796 [==============================] - 146s - loss: 8907.7040 - val_loss: 5788.8614\n",
      "Epoch 10/35\n",
      "76796/76796 [==============================] - 146s - loss: 8807.2780 - val_loss: 5718.9640\n",
      "Epoch 11/35\n",
      "76796/76796 [==============================] - 145s - loss: 8646.1518 - val_loss: 5524.2761\n",
      "Epoch 12/35\n",
      "76796/76796 [==============================] - 145s - loss: 8468.0482 - val_loss: 5456.4008\n",
      "Epoch 13/35\n",
      "76796/76796 [==============================] - 145s - loss: 8328.9032 - val_loss: 5274.8272\n",
      "Epoch 14/35\n",
      "76796/76796 [==============================] - 145s - loss: 8175.5070 - val_loss: 5127.8153\n",
      "Epoch 15/35\n",
      "76796/76796 [==============================] - 145s - loss: 8094.2276 - val_loss: 5248.7073\n",
      "Epoch 16/35\n",
      "76796/76796 [==============================] - 145s - loss: 7961.3261 - val_loss: 5127.5275\n",
      "Epoch 17/35\n",
      "76796/76796 [==============================] - 145s - loss: 7910.6706 - val_loss: 4933.0410\n",
      "Epoch 18/35\n",
      "76796/76796 [==============================] - 145s - loss: 7854.9603 - val_loss: 4915.1154\n",
      "Epoch 19/35\n",
      "76796/76796 [==============================] - 145s - loss: 7836.7134 - val_loss: 4859.1486\n",
      "Epoch 20/35\n",
      "76796/76796 [==============================] - 145s - loss: 7821.8868 - val_loss: 4880.9116\n",
      "Epoch 21/35\n",
      "76796/76796 [==============================] - 145s - loss: 7759.5270 - val_loss: 4937.9341\n",
      "Epoch 22/35\n",
      "76796/76796 [==============================] - 145s - loss: 7706.4979 - val_loss: 4866.2938\n",
      "Epoch 23/35\n",
      "76796/76796 [==============================] - 145s - loss: 7661.3977 - val_loss: 4760.7040\n",
      "Epoch 24/35\n",
      "76796/76796 [==============================] - 145s - loss: 7655.8762 - val_loss: 4761.6139\n",
      "Epoch 25/35\n",
      "76796/76796 [==============================] - 145s - loss: 7632.6630 - val_loss: 4817.1028\n",
      "Epoch 26/35\n",
      "76796/76796 [==============================] - 145s - loss: 7575.6240 - val_loss: 4764.1703\n",
      "Epoch 27/35\n",
      "76796/76796 [==============================] - 145s - loss: 7569.1694 - val_loss: 4850.8236\n",
      "Epoch 28/35\n",
      "76796/76796 [==============================] - 145s - loss: 7564.3062 - val_loss: 4765.7377\n",
      "Epoch 29/35\n",
      "76796/76796 [==============================] - 146s - loss: 7572.5786 - val_loss: 4926.5521\n",
      "Epoch 30/35\n",
      "76796/76796 [==============================] - 146s - loss: 7468.1937 - val_loss: 4725.3528\n",
      "Epoch 31/35\n",
      "76796/76796 [==============================] - 146s - loss: 7463.2669 - val_loss: 4951.1801\n",
      "Epoch 32/35\n",
      "76796/76796 [==============================] - 147s - loss: 7497.6844 - val_loss: 4743.0922\n",
      "Epoch 33/35\n",
      "76796/76796 [==============================] - 146s - loss: 7399.3070 - val_loss: 4702.1234\n",
      "Epoch 34/35\n",
      "76796/76796 [==============================] - 146s - loss: 7412.2034 - val_loss: 4652.1605\n",
      "Epoch 35/35\n",
      "76796/76796 [==============================] - 146s - loss: 7415.8879 - val_loss: 4833.3607\n",
      "Percent of variability explained by model: 39.1430997174\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF27',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18176 samples, validate on 4545 samples\n",
      "Epoch 1/35\n",
      "18176/18176 [==============================] - 36s - loss: 4012.5159 - val_loss: 7084.5936\n",
      "Epoch 2/35\n",
      "18176/18176 [==============================] - 34s - loss: 3566.8719 - val_loss: 6579.0753\n",
      "Epoch 3/35\n",
      "18176/18176 [==============================] - 34s - loss: 3286.6815 - val_loss: 6295.5785\n",
      "Epoch 4/35\n",
      "18176/18176 [==============================] - 34s - loss: 3247.5279 - val_loss: 6296.0166\n",
      "Epoch 5/35\n",
      "18176/18176 [==============================] - 34s - loss: 3208.5870 - val_loss: 6249.1865\n",
      "Epoch 6/35\n",
      "18176/18176 [==============================] - 34s - loss: 3190.5696 - val_loss: 6212.4715\n",
      "Epoch 7/35\n",
      "18176/18176 [==============================] - 34s - loss: 3186.3926 - val_loss: 6198.6038\n",
      "Epoch 8/35\n",
      "18176/18176 [==============================] - 35s - loss: 3160.3990 - val_loss: 6161.8641\n",
      "Epoch 9/35\n",
      "18176/18176 [==============================] - 34s - loss: 3126.4145 - val_loss: 6011.4645\n",
      "Epoch 10/35\n",
      "18176/18176 [==============================] - 34s - loss: 3067.4938 - val_loss: 6045.6275\n",
      "Epoch 11/35\n",
      "18176/18176 [==============================] - 34s - loss: 3007.7921 - val_loss: 6069.6114\n",
      "Epoch 12/35\n",
      "18176/18176 [==============================] - 34s - loss: 2922.9409 - val_loss: 5861.1404\n",
      "Epoch 13/35\n",
      "18176/18176 [==============================] - 34s - loss: 2859.7227 - val_loss: 5642.7361\n",
      "Epoch 14/35\n",
      "18176/18176 [==============================] - 34s - loss: 2825.4111 - val_loss: 5586.3013\n",
      "Epoch 15/35\n",
      "18176/18176 [==============================] - 34s - loss: 2772.2867 - val_loss: 5546.1216\n",
      "Epoch 16/35\n",
      "18176/18176 [==============================] - 34s - loss: 2731.9619 - val_loss: 5508.5621\n",
      "Epoch 17/35\n",
      "18176/18176 [==============================] - 34s - loss: 2683.8070 - val_loss: 5549.8098\n",
      "Epoch 18/35\n",
      "18176/18176 [==============================] - 34s - loss: 2658.1472 - val_loss: 5618.1533\n",
      "Epoch 19/35\n",
      "18176/18176 [==============================] - 34s - loss: 2604.2639 - val_loss: 5572.4899\n",
      "Epoch 20/35\n",
      "18176/18176 [==============================] - 34s - loss: 2589.8870 - val_loss: 5391.9571\n",
      "Epoch 21/35\n",
      "18176/18176 [==============================] - 34s - loss: 2550.2193 - val_loss: 5328.2595\n",
      "Epoch 22/35\n",
      "18176/18176 [==============================] - 35s - loss: 2525.8761 - val_loss: 5395.4145\n",
      "Epoch 23/35\n",
      "18176/18176 [==============================] - 34s - loss: 2507.5790 - val_loss: 5364.2836\n",
      "Epoch 24/35\n",
      "18176/18176 [==============================] - 34s - loss: 2470.4161 - val_loss: 5881.2107\n",
      "Epoch 25/35\n",
      "18176/18176 [==============================] - 34s - loss: 2485.9862 - val_loss: 5308.3448\n",
      "Epoch 26/35\n",
      "18176/18176 [==============================] - 34s - loss: 2448.1507 - val_loss: 5222.3359\n",
      "Epoch 27/35\n",
      "18176/18176 [==============================] - 34s - loss: 2431.3644 - val_loss: 5220.8918\n",
      "Epoch 28/35\n",
      "18176/18176 [==============================] - 34s - loss: 2407.7872 - val_loss: 5316.2696\n",
      "Epoch 29/35\n",
      "18176/18176 [==============================] - 34s - loss: 2402.7066 - val_loss: 5425.3541\n",
      "Epoch 30/35\n",
      "18176/18176 [==============================] - 34s - loss: 2374.2633 - val_loss: 5628.7410\n",
      "Epoch 31/35\n",
      "18176/18176 [==============================] - 34s - loss: 2362.0490 - val_loss: 5157.7844\n",
      "Epoch 32/35\n",
      "18176/18176 [==============================] - 34s - loss: 2350.6472 - val_loss: 5112.2845\n",
      "Epoch 33/35\n",
      "18176/18176 [==============================] - 34s - loss: 2320.0966 - val_loss: 5099.1438\n",
      "Epoch 34/35\n",
      "18176/18176 [==============================] - 34s - loss: 2320.7829 - val_loss: 5319.6228\n",
      "Epoch 35/35\n",
      "18176/18176 [==============================] - 34s - loss: 2307.0901 - val_loss: 5076.8829\n",
      "Percent of variability explained by model: 27.0574528136\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF29',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143977 samples, validate on 35995 samples\n",
      "Epoch 1/35\n",
      "143977/143977 [==============================] - 285s - loss: 7579.5630 - val_loss: 6716.8616\n",
      "Epoch 2/35\n",
      "143977/143977 [==============================] - 283s - loss: 6900.0216 - val_loss: 6101.3160\n",
      "Epoch 3/35\n",
      "143977/143977 [==============================] - 283s - loss: 6460.5100 - val_loss: 5660.7465\n",
      "Epoch 4/35\n",
      "143977/143977 [==============================] - 279s - loss: 6161.7054 - val_loss: 5412.6841\n",
      "Epoch 5/35\n",
      "143977/143977 [==============================] - 278s - loss: 5891.0374 - val_loss: 5073.5241\n",
      "Epoch 6/35\n",
      "143977/143977 [==============================] - 273s - loss: 5677.0033 - val_loss: 5209.0427\n",
      "Epoch 7/35\n",
      "143977/143977 [==============================] - 272s - loss: 5523.8373 - val_loss: 5087.7659\n",
      "Epoch 8/35\n",
      "143977/143977 [==============================] - 272s - loss: 5443.0108 - val_loss: 4654.2568\n",
      "Epoch 9/35\n",
      "143977/143977 [==============================] - 272s - loss: 5378.5422 - val_loss: 4784.9655\n",
      "Epoch 10/35\n",
      "143977/143977 [==============================] - 272s - loss: 5308.0740 - val_loss: 4685.2169\n",
      "Epoch 11/35\n",
      "143977/143977 [==============================] - 272s - loss: 5274.7925 - val_loss: 4485.4553\n",
      "Epoch 12/35\n",
      "143977/143977 [==============================] - 272s - loss: 5240.1961 - val_loss: 4404.9086\n",
      "Epoch 13/35\n",
      "143977/143977 [==============================] - 271s - loss: 5174.4818 - val_loss: 4434.3932\n",
      "Epoch 14/35\n",
      "143977/143977 [==============================] - 271s - loss: 5156.3506 - val_loss: 4378.4646\n",
      "Epoch 15/35\n",
      "143977/143977 [==============================] - 269s - loss: 5100.1135 - val_loss: 4388.5885\n",
      "Epoch 16/35\n",
      "143977/143977 [==============================] - 271s - loss: 5125.0379 - val_loss: 4539.8653\n",
      "Epoch 17/35\n",
      "143977/143977 [==============================] - 270s - loss: 5070.6540 - val_loss: 4318.6163\n",
      "Epoch 18/35\n",
      "143977/143977 [==============================] - 270s - loss: 5047.2464 - val_loss: 4314.6800\n",
      "Epoch 19/35\n",
      "143977/143977 [==============================] - 270s - loss: 5051.9157 - val_loss: 4853.8329\n",
      "Epoch 20/35\n",
      "143977/143977 [==============================] - 271s - loss: 5017.2338 - val_loss: 4319.0373\n",
      "Epoch 21/35\n",
      "143977/143977 [==============================] - 270s - loss: 4976.4195 - val_loss: 4525.0484\n",
      "Epoch 22/35\n",
      "143977/143977 [==============================] - 271s - loss: 5013.3117 - val_loss: 4302.9194\n",
      "Epoch 23/35\n",
      "143977/143977 [==============================] - 270s - loss: 4988.6097 - val_loss: 4322.3924\n",
      "Epoch 24/35\n",
      "143977/143977 [==============================] - 271s - loss: 4968.3341 - val_loss: 4360.5668\n",
      "Epoch 25/35\n",
      "143977/143977 [==============================] - 272s - loss: 4950.3984 - val_loss: 4367.1609\n",
      "Epoch 26/35\n",
      "143977/143977 [==============================] - 271s - loss: 4943.4135 - val_loss: 4348.8226\n",
      "Epoch 27/35\n",
      "143977/143977 [==============================] - 271s - loss: 4919.3387 - val_loss: 4241.8603\n",
      "Epoch 28/35\n",
      "143977/143977 [==============================] - 270s - loss: 4966.2171 - val_loss: 4456.0095\n",
      "Epoch 29/35\n",
      "143977/143977 [==============================] - 271s - loss: 4958.5742 - val_loss: 4428.8874\n",
      "Epoch 30/35\n",
      "143977/143977 [==============================] - 271s - loss: 4944.5526 - val_loss: 4380.0534\n",
      "Epoch 31/35\n",
      "143977/143977 [==============================] - 271s - loss: 4952.8635 - val_loss: 4304.8828\n",
      "Epoch 32/35\n",
      "143977/143977 [==============================] - 271s - loss: 4946.5621 - val_loss: 4371.4260\n",
      "Epoch 33/35\n",
      "143977/143977 [==============================] - 273s - loss: 4937.9862 - val_loss: 4248.7575\n",
      "Epoch 34/35\n",
      "143977/143977 [==============================] - 285s - loss: 4931.1894 - val_loss: 4442.6600\n",
      "Epoch 35/35\n",
      "143977/143977 [==============================] - 283s - loss: 4972.1078 - val_loss: 4327.4686\n",
      "Percent of variability explained by model: 35.8951275737\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF34',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11855 samples, validate on 2964 samples\n",
      "Epoch 1/35\n",
      "11855/11855 [==============================] - 24s - loss: 6097.4915 - val_loss: 5784.4525\n",
      "Epoch 2/35\n",
      "11855/11855 [==============================] - 23s - loss: 5772.8231 - val_loss: 5463.0940\n",
      "Epoch 3/35\n",
      "11855/11855 [==============================] - 23s - loss: 5440.4093 - val_loss: 5099.5840\n",
      "Epoch 4/35\n",
      "11855/11855 [==============================] - 23s - loss: 5102.9008 - val_loss: 4760.2303\n",
      "Epoch 5/35\n",
      "11855/11855 [==============================] - 23s - loss: 4855.0404 - val_loss: 4536.7182\n",
      "Epoch 6/35\n",
      "11855/11855 [==============================] - 23s - loss: 4734.2198 - val_loss: 4467.9315\n",
      "Epoch 7/35\n",
      "11855/11855 [==============================] - 23s - loss: 4741.3306 - val_loss: 4454.6128\n",
      "Epoch 8/35\n",
      "11855/11855 [==============================] - 23s - loss: 4697.8945 - val_loss: 4430.3923\n",
      "Epoch 9/35\n",
      "11855/11855 [==============================] - 23s - loss: 4688.1672 - val_loss: 4399.4323\n",
      "Epoch 10/35\n",
      "11855/11855 [==============================] - 23s - loss: 4661.9672 - val_loss: 4423.1073\n",
      "Epoch 11/35\n",
      "11855/11855 [==============================] - 23s - loss: 4643.8215 - val_loss: 4335.0464\n",
      "Epoch 12/35\n",
      "11855/11855 [==============================] - 23s - loss: 4646.6278 - val_loss: 4333.4177\n",
      "Epoch 13/35\n",
      "11855/11855 [==============================] - 23s - loss: 4625.0273 - val_loss: 4321.1520\n",
      "Epoch 14/35\n",
      "11855/11855 [==============================] - 23s - loss: 4583.0405 - val_loss: 4285.6209\n",
      "Epoch 15/35\n",
      "11855/11855 [==============================] - 23s - loss: 4605.0021 - val_loss: 4297.5199\n",
      "Epoch 16/35\n",
      "11855/11855 [==============================] - 23s - loss: 4566.4320 - val_loss: 4237.5957\n",
      "Epoch 17/35\n",
      "11855/11855 [==============================] - 23s - loss: 4543.7511 - val_loss: 4273.7716\n",
      "Epoch 18/35\n",
      "11855/11855 [==============================] - 23s - loss: 4544.9670 - val_loss: 4163.1217\n",
      "Epoch 19/35\n",
      "11855/11855 [==============================] - 23s - loss: 4509.9262 - val_loss: 4199.9296\n",
      "Epoch 20/35\n",
      "11855/11855 [==============================] - 23s - loss: 4483.4442 - val_loss: 4067.7207\n",
      "Epoch 21/35\n",
      "11855/11855 [==============================] - 23s - loss: 4448.4036 - val_loss: 4045.7609\n",
      "Epoch 22/35\n",
      "11855/11855 [==============================] - 23s - loss: 4409.2989 - val_loss: 3975.7726\n",
      "Epoch 23/35\n",
      "11855/11855 [==============================] - 23s - loss: 4329.2529 - val_loss: 4013.4743\n",
      "Epoch 24/35\n",
      "11855/11855 [==============================] - 23s - loss: 4335.6587 - val_loss: 3878.5343\n",
      "Epoch 25/35\n",
      "11855/11855 [==============================] - 23s - loss: 4279.5517 - val_loss: 3804.0455\n",
      "Epoch 26/35\n",
      "11855/11855 [==============================] - 23s - loss: 4232.8491 - val_loss: 3914.8966\n",
      "Epoch 27/35\n",
      "11855/11855 [==============================] - 23s - loss: 4159.4006 - val_loss: 3857.0168\n",
      "Epoch 28/35\n",
      "11855/11855 [==============================] - 23s - loss: 4125.4522 - val_loss: 3911.7196\n",
      "Epoch 29/35\n",
      "11855/11855 [==============================] - 23s - loss: 4082.1091 - val_loss: 3632.5541\n",
      "Epoch 30/35\n",
      "11855/11855 [==============================] - 23s - loss: 4106.7159 - val_loss: 3781.8576\n",
      "Epoch 31/35\n",
      "11855/11855 [==============================] - 23s - loss: 4043.0479 - val_loss: 3584.0946\n",
      "Epoch 32/35\n",
      "11855/11855 [==============================] - 23s - loss: 4046.0540 - val_loss: 4309.6445\n",
      "Epoch 33/35\n",
      "11855/11855 [==============================] - 23s - loss: 3999.6126 - val_loss: 3581.3163\n",
      "Epoch 34/35\n",
      "11855/11855 [==============================] - 23s - loss: 3946.7305 - val_loss: 3549.4471\n",
      "Epoch 35/35\n",
      "11855/11855 [==============================] - 23s - loss: 3944.2907 - val_loss: 3476.7079\n",
      "Percent of variability explained by model: 25.1822268533\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF35',35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13050 samples, validate on 3263 samples\n",
      "Epoch 1/35\n",
      "13050/13050 [==============================] - 26s - loss: 7428.7201 - val_loss: 7121.2714\n",
      "Epoch 2/35\n",
      "13050/13050 [==============================] - 25s - loss: 6905.6091 - val_loss: 6518.6309\n",
      "Epoch 3/35\n",
      "13050/13050 [==============================] - 25s - loss: 6401.2708 - val_loss: 6051.2632\n",
      "Epoch 4/35\n",
      "13050/13050 [==============================] - 25s - loss: 6067.2285 - val_loss: 5770.6936\n",
      "Epoch 5/35\n",
      "13050/13050 [==============================] - 25s - loss: 5732.0044 - val_loss: 5313.6510\n",
      "Epoch 6/35\n",
      "13050/13050 [==============================] - 25s - loss: 5421.6541 - val_loss: 4916.7067\n",
      "Epoch 7/35\n",
      "13050/13050 [==============================] - 26s - loss: 5175.1188 - val_loss: 4663.0209\n",
      "Epoch 8/35\n",
      "13050/13050 [==============================] - 25s - loss: 4952.7396 - val_loss: 4365.2324\n",
      "Epoch 9/35\n",
      "13050/13050 [==============================] - 25s - loss: 4755.1789 - val_loss: 4247.4092\n",
      "Epoch 10/35\n",
      "13050/13050 [==============================] - 25s - loss: 4596.4414 - val_loss: 4046.7987\n",
      "Epoch 11/35\n",
      "13050/13050 [==============================] - 26s - loss: 4411.2367 - val_loss: 4019.5627\n",
      "Epoch 12/35\n",
      "13050/13050 [==============================] - 26s - loss: 4339.9014 - val_loss: 3774.2913\n",
      "Epoch 13/35\n",
      "13050/13050 [==============================] - 26s - loss: 4167.6949 - val_loss: 3708.8584\n",
      "Epoch 14/35\n",
      "13050/13050 [==============================] - 25s - loss: 4008.3335 - val_loss: 3444.2087\n",
      "Epoch 15/35\n",
      "13050/13050 [==============================] - 25s - loss: 3858.9158 - val_loss: 3276.1083\n",
      "Epoch 16/35\n",
      "13050/13050 [==============================] - 25s - loss: 3762.9474 - val_loss: 3232.8461\n",
      "Epoch 17/35\n",
      "13050/13050 [==============================] - 25s - loss: 3689.1088 - val_loss: 3418.8968\n",
      "Epoch 18/35\n",
      "13050/13050 [==============================] - 25s - loss: 3656.7790 - val_loss: 3084.6887\n",
      "Epoch 19/35\n",
      "13050/13050 [==============================] - 25s - loss: 3494.1276 - val_loss: 3005.5934\n",
      "Epoch 20/35\n",
      "13050/13050 [==============================] - 25s - loss: 3515.4422 - val_loss: 3321.5681\n",
      "Epoch 21/35\n",
      "13050/13050 [==============================] - 24s - loss: 3500.3236 - val_loss: 2941.3580\n",
      "Epoch 22/35\n",
      "13050/13050 [==============================] - 24s - loss: 3438.3702 - val_loss: 2966.1284\n",
      "Epoch 23/35\n",
      "13050/13050 [==============================] - 24s - loss: 3421.8837 - val_loss: 2891.6533\n",
      "Epoch 24/35\n",
      "13050/13050 [==============================] - 24s - loss: 3355.9554 - val_loss: 3008.0930\n",
      "Epoch 25/35\n",
      "13050/13050 [==============================] - 24s - loss: 3318.6918 - val_loss: 2890.2977\n",
      "Epoch 26/35\n",
      "13050/13050 [==============================] - 24s - loss: 3292.5316 - val_loss: 2829.5120\n",
      "Epoch 27/35\n",
      "13050/13050 [==============================] - 24s - loss: 3330.8081 - val_loss: 2779.3205\n",
      "Epoch 28/35\n",
      "13050/13050 [==============================] - 24s - loss: 3240.6894 - val_loss: 2909.2865\n",
      "Epoch 29/35\n",
      "13050/13050 [==============================] - 24s - loss: 3274.1068 - val_loss: 2819.2848\n",
      "Epoch 30/35\n",
      "13050/13050 [==============================] - 24s - loss: 3231.6703 - val_loss: 2715.2253\n",
      "Epoch 31/35\n",
      "13050/13050 [==============================] - 25s - loss: 3188.5408 - val_loss: 2798.0564\n",
      "Epoch 32/35\n",
      "13050/13050 [==============================] - 24s - loss: 3151.7353 - val_loss: 2663.8983\n",
      "Epoch 33/35\n",
      "13050/13050 [==============================] - 25s - loss: 3148.6593 - val_loss: 2674.2856\n",
      "Epoch 34/35\n",
      "13050/13050 [==============================] - 24s - loss: 3167.8683 - val_loss: 2657.1494\n",
      "Epoch 35/35\n",
      "13050/13050 [==============================] - 24s - loss: 3064.3372 - val_loss: 2570.6496\n",
      "Percent of variability explained by model: 55.419082283\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model('ARF39',35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset without a negative set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Bio/Seq.py:341: BiopythonDeprecationWarning: This method is obsolete; please use str(my_seq) instead of my_seq.tostring().\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF4_GEM_events.txt','ARF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF39_GEM_events.txt','ARF39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF35_GEM_events.txt','ARF35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF34_GEM_events.txt','ARF34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF10_GEM_events.txt','ARF10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF13_GEM_events.txt','ARF13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF16_GEM_events.txt','ARF16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF18_GEM_events.txt','ARF18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF27_GEM_events.txt','ARF27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generate_training_and_test_datasets_no_negative('/mnt/Data_DapSeq_Maize/ARF29_GEM_events.txt','ARF29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5747\n",
      "6713\n",
      "3672\n",
      "8429\n",
      "1490\n",
      "12000\n",
      "2841\n",
      "22497\n",
      "1853\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "#finding the min length of the test set\n",
    "List_of_ARFs =['ARF4','ARF10','ARF13','ARF16','ARF18','ARF27','ARF29','ARF34','ARF35','ARF39']\n",
    "seq_test_sets = [None]*len(List_of_ARFs)\n",
    "\n",
    "counter1=0\n",
    "for ARF_label in List_of_ARFs:\n",
    "    seq_test_sets[counter1]=numpy.load('/mnt/Data_DapSeq_Maize/'+ARF_label+'no_negative_seq_test.npy')\n",
    "    print(len(seq_test_sets[counter1]))\n",
    "    counter1=counter1+1\n",
    "\n",
    "#based on this the test set will only be: 5960 in size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 5890.5125 - val_loss: 3614.3051\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 5528.7237 - val_loss: 3294.9960\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 5235.2277 - val_loss: 3047.9870\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 4984.3632 - val_loss: 2803.1345\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 4736.9652 - val_loss: 2548.9849\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 4460.6861 - val_loss: 2294.1966\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 4191.2999 - val_loss: 2046.2299\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 3953.9948 - val_loss: 1813.1747\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 3738.8059 - val_loss: 1598.4614\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 3529.8368 - val_loss: 1408.6134\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 3407.1694 - val_loss: 1258.9088\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 3251.3649 - val_loss: 1152.9603\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 3157.0674 - val_loss: 1093.9995\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 3131.3119 - val_loss: 1069.0722\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 3116.3295 - val_loss: 1061.3519\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 3109.9465 - val_loss: 1057.1305\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 3110.8765 - val_loss: 1056.5939\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 8s - loss: 3105.8370 - val_loss: 1056.4659\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 3062.0590 - val_loss: 1053.6128\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 3108.6857 - val_loss: 1053.4548\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 8s - loss: 3133.4301 - val_loss: 1053.0593\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 8s - loss: 3113.1641 - val_loss: 1046.6179\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 8s - loss: 3074.2523 - val_loss: 1045.2557\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 8s - loss: 3069.8055 - val_loss: 1039.6831\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 3076.5949 - val_loss: 1032.0653\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 3070.1830 - val_loss: 1032.2664\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 8s - loss: 3115.4764 - val_loss: 1040.6073\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 8s - loss: 3080.5920 - val_loss: 1030.9438\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 8s - loss: 3062.3926 - val_loss: 1018.2940\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 8s - loss: 3092.7998 - val_loss: 1020.9360\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 8s - loss: 3000.1942 - val_loss: 1035.0298\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 3081.3833 - val_loss: 1026.1282\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 8s - loss: 3024.4947 - val_loss: 1015.7370\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 8s - loss: 3050.8801 - val_loss: 1006.7533\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 8s - loss: 3063.3125 - val_loss: 1017.8633\n",
      "Percent of variability explained by model: 3.19509738443\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF4',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 14577.9416 - val_loss: 12933.7465\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 14073.1560 - val_loss: 12504.9977\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 13627.9211 - val_loss: 12106.2705\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 13168.9152 - val_loss: 11632.3015\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 12722.4319 - val_loss: 11137.6340\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 12167.5643 - val_loss: 10612.4973\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 11630.5282 - val_loss: 10079.3457\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 11041.3092 - val_loss: 9539.3172\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 10532.2490 - val_loss: 9018.0391\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 9952.3070 - val_loss: 8501.5769\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 9422.7393 - val_loss: 8023.5584\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 8998.2327 - val_loss: 7595.9637\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 8575.4874 - val_loss: 7229.7785\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 8399.7434 - val_loss: 6945.9726\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 8145.4583 - val_loss: 6750.0343\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 7946.3254 - val_loss: 6634.4909\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 7875.7046 - val_loss: 6585.3010\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 7867.9213 - val_loss: 6560.7160\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 7687.4446 - val_loss: 6336.8109\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 7533.5556 - val_loss: 6030.5207\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 7259.3354 - val_loss: 5880.5821\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 7227.7054 - val_loss: 5729.5056\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 7139.2910 - val_loss: 5626.4758\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 6972.7025 - val_loss: 5523.7227\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 6826.7708 - val_loss: 5404.3682\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 6734.6967 - val_loss: 5301.3958\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 6646.6681 - val_loss: 5180.1779\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 6548.8659 - val_loss: 5105.7130\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 6489.3590 - val_loss: 5014.1606\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 6177.1732 - val_loss: 4878.6666\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 6114.8994 - val_loss: 4786.8649\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 6014.9641 - val_loss: 4847.1161\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 5702.4390 - val_loss: 4607.3779\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 5764.0252 - val_loss: 4522.6867\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 5857.0698 - val_loss: 4632.0016\n",
      "Percent of variability explained by model: 29.5327329983\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF39',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 10946.4969 - val_loss: 11185.2873\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 10533.5791 - val_loss: 10834.7799\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 10172.1700 - val_loss: 10463.8320\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 9768.0352 - val_loss: 10055.2487\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 9382.1408 - val_loss: 9659.6886\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 8987.8170 - val_loss: 9230.7290\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 8547.3093 - val_loss: 8785.8348\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 8088.5609 - val_loss: 8313.3311\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 7666.6547 - val_loss: 7840.2453\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 7177.4912 - val_loss: 7360.9617\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 6750.3059 - val_loss: 6913.5058\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 6412.9564 - val_loss: 6489.9776\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 8s - loss: 5985.2282 - val_loss: 6098.3254\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 8s - loss: 5702.4702 - val_loss: 5778.1726\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 5466.2012 - val_loss: 5513.1851\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 8s - loss: 5194.3979 - val_loss: 5334.8031\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 8s - loss: 5223.3418 - val_loss: 5231.1662\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 8s - loss: 5209.4964 - val_loss: 5190.8448\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 8s - loss: 5214.6889 - val_loss: 5174.3212\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 8s - loss: 5162.2242 - val_loss: 5167.8655\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 8s - loss: 5167.8035 - val_loss: 5163.2450\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 8s - loss: 5150.0007 - val_loss: 5165.4142\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 8s - loss: 5127.5728 - val_loss: 5165.7103\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 8s - loss: 5161.7497 - val_loss: 5152.7857\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 5093.5866 - val_loss: 5156.1450\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 8s - loss: 5144.9233 - val_loss: 5149.2959\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 5120.0145 - val_loss: 5146.1774\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 8s - loss: 5100.5471 - val_loss: 5127.4838\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 8s - loss: 5218.7795 - val_loss: 5133.8482\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 8s - loss: 5057.8604 - val_loss: 5109.0342\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 8s - loss: 5174.0485 - val_loss: 5055.1585\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 5060.4608 - val_loss: 5053.0450\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 5055.0064 - val_loss: 5079.4916\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 8s - loss: 5064.3328 - val_loss: 5132.8067\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 8s - loss: 5078.3729 - val_loss: 5052.6419\n",
      "Percent of variability explained by model: 1.64739355478\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF35',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 17719.5891 - val_loss: 15691.4057\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 17452.0443 - val_loss: 15405.6035\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 17168.2036 - val_loss: 15148.4330\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 8s - loss: 16901.5397 - val_loss: 14895.5741\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 8s - loss: 16628.1404 - val_loss: 14619.2694\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 8s - loss: 16361.8708 - val_loss: 14326.8482\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 8s - loss: 16051.8572 - val_loss: 14009.0199\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 15706.0098 - val_loss: 13676.6697\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 15397.3244 - val_loss: 13333.9494\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 15009.6023 - val_loss: 12979.2436\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 14674.8238 - val_loss: 12600.9859\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 8s - loss: 14267.0471 - val_loss: 12206.1986\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 8s - loss: 13848.2059 - val_loss: 11788.0071\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 13471.5364 - val_loss: 11370.2029\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 13076.1669 - val_loss: 10950.5317\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 12676.4646 - val_loss: 10537.8259\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 12326.4688 - val_loss: 10144.1147\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 11923.4663 - val_loss: 9786.9047\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 11610.7356 - val_loss: 9465.7874\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 11454.2503 - val_loss: 9196.2210\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 11246.4061 - val_loss: 9008.6464\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 11091.3923 - val_loss: 8859.4581\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 11032.3310 - val_loss: 8764.4206\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 10998.9423 - val_loss: 8719.2872\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 8s - loss: 10795.6257 - val_loss: 8693.3548\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 10882.1229 - val_loss: 8686.4291\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 10984.9797 - val_loss: 8672.6397\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 10812.2505 - val_loss: 8653.8552\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 10933.0021 - val_loss: 8626.3384\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 10920.2750 - val_loss: 8619.0407\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 10941.7363 - val_loss: 8649.7234\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 10941.4361 - val_loss: 8625.5589\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 10804.4650 - val_loss: 8610.9649\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 10757.0189 - val_loss: 8592.7522\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 10730.3960 - val_loss: 8608.0205\n",
      "Percent of variability explained by model: 1.52459054695\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF34',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 10s - loss: 6038.4743 - val_loss: 5441.5113\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 6013.6423 - val_loss: 5409.8628\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 5957.5842 - val_loss: 5327.1531\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 5844.2286 - val_loss: 5188.5432\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 5695.9725 - val_loss: 5036.9625\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 5543.7034 - val_loss: 4894.5348\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 5386.9951 - val_loss: 4749.1138\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 5228.7694 - val_loss: 4593.0795\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 5060.7379 - val_loss: 4424.5420\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 4878.3037 - val_loss: 4248.0325\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 4689.4169 - val_loss: 4060.8073\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 4497.5811 - val_loss: 3858.1250\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 4295.6912 - val_loss: 3647.7340\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 4086.2889 - val_loss: 3434.6174\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 3855.9629 - val_loss: 3221.7091\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 3623.0568 - val_loss: 3008.6255\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 3423.5171 - val_loss: 2802.5752\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 3230.9153 - val_loss: 2609.9946\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 3070.5212 - val_loss: 2429.6158\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 2873.4575 - val_loss: 2264.9523\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 2744.0030 - val_loss: 2126.4487\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 2638.5610 - val_loss: 2011.8129\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 2555.4632 - val_loss: 1931.4408\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 2442.8877 - val_loss: 1877.8695\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 2439.2317 - val_loss: 1851.3058\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 2448.2587 - val_loss: 1842.5093\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 2459.8678 - val_loss: 1838.9713\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 2423.0409 - val_loss: 1838.3413\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 2418.2878 - val_loss: 1837.0333\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 2426.1064 - val_loss: 1835.6744\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 2391.2237 - val_loss: 1834.8731\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 2372.5840 - val_loss: 1835.0356\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 2408.0459 - val_loss: 1744.9750\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 2290.0327 - val_loss: 1707.1783\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 2276.2436 - val_loss: 1650.7425\n",
      "Percent of variability explained by model: 14.6807874861\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF10',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 7162.8012 - val_loss: 7485.3040\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 6905.1397 - val_loss: 7258.5765\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 6696.2349 - val_loss: 7051.5224\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 6462.5410 - val_loss: 6790.3744\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 6218.5152 - val_loss: 6525.9009\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 5944.5448 - val_loss: 6231.9420\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 5665.5727 - val_loss: 5918.8718\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 5383.3247 - val_loss: 5582.5464\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 5059.8562 - val_loss: 5234.5013\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 4765.1292 - val_loss: 4879.9716\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 4403.7091 - val_loss: 4524.2552\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 4133.4844 - val_loss: 4173.1508\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 3805.9474 - val_loss: 3835.1713\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 3531.3655 - val_loss: 3523.2739\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 3312.8969 - val_loss: 3247.8757\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 3100.3144 - val_loss: 3001.7607\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 2919.4300 - val_loss: 2809.6163\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 2863.8568 - val_loss: 2591.3716\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 2701.4685 - val_loss: 2455.6865\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 2682.5971 - val_loss: 2385.1186\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 2550.9321 - val_loss: 2284.0103\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 2494.7227 - val_loss: 2136.7737\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 2444.3440 - val_loss: 2023.9215\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 2393.5707 - val_loss: 1976.5227\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 2353.7849 - val_loss: 1905.7492\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 2347.7937 - val_loss: 1895.6703\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 2313.1156 - val_loss: 1909.8415\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 2268.2340 - val_loss: 1843.8834\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 2258.2211 - val_loss: 1847.6030\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 2259.9826 - val_loss: 1826.7762\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 2222.8086 - val_loss: 1789.6405\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 2185.0375 - val_loss: 1777.4582\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 2217.3778 - val_loss: 1773.5324\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 2193.6455 - val_loss: 1745.4401\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 2146.9611 - val_loss: 1718.1591\n",
      "Percent of variability explained by model: 33.7417860049\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF13',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 19244.4914 - val_loss: 24385.7134\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 18528.0734 - val_loss: 23616.8646\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 17900.6331 - val_loss: 23054.1207\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 17397.3471 - val_loss: 22536.2846\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 16954.8024 - val_loss: 21986.4600\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 16358.7515 - val_loss: 21410.0441\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 15858.3919 - val_loss: 20821.5654\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 15400.5026 - val_loss: 20228.1970\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 14880.9121 - val_loss: 19647.1964\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 14325.1711 - val_loss: 19093.4150\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 13781.4194 - val_loss: 18552.1781\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 13409.9945 - val_loss: 18077.7874\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 13096.8066 - val_loss: 17643.4105\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 12821.4105 - val_loss: 17279.7092\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 12589.8869 - val_loss: 16990.8248\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 12370.1985 - val_loss: 16774.7517\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 12473.5209 - val_loss: 16664.1146\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 12148.5408 - val_loss: 16627.3970\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 13136.7808 - val_loss: 17018.8270\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 12281.1203 - val_loss: 16429.5476\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 12374.0057 - val_loss: 16452.0995\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 12418.8376 - val_loss: 16509.9579\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 12208.7118 - val_loss: 16509.8445\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 12421.2617 - val_loss: 16568.2082\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 12307.4629 - val_loss: 16488.2786\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 12321.5856 - val_loss: 16469.5076\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 12337.3969 - val_loss: 16462.5404\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 12169.4150 - val_loss: 16444.9535\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 12219.2012 - val_loss: 16488.7667\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 12161.6512 - val_loss: 16358.9330\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 12145.7816 - val_loss: 16399.7615\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 12129.2868 - val_loss: 16412.4780\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 12153.1283 - val_loss: 16354.2944\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 12067.0772 - val_loss: 16388.0742\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 12306.4900 - val_loss: 16252.7368\n",
      "Percent of variability explained by model: 1.59533134158\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF16',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 8748.9681 - val_loss: 8777.8464\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 8583.0339 - val_loss: 8566.4760\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 8386.2066 - val_loss: 8387.5742\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 8198.7488 - val_loss: 8203.5701\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 7990.8369 - val_loss: 7997.7229\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 7784.3298 - val_loss: 7772.3799\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 7540.8908 - val_loss: 7531.1038\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 7290.9073 - val_loss: 7271.7973\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 7025.3031 - val_loss: 7002.6395\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 6727.2333 - val_loss: 6721.8100\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 6422.1877 - val_loss: 6437.5237\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 6160.3896 - val_loss: 6152.1863\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 5862.7063 - val_loss: 5840.1146\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 5571.1472 - val_loss: 5537.3236\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 5255.9096 - val_loss: 5244.6692\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 4968.9115 - val_loss: 4969.2486\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 4731.7806 - val_loss: 4711.6057\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 4502.8601 - val_loss: 4483.1978\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 4349.1414 - val_loss: 4294.9318\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 4153.2301 - val_loss: 4147.4267\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 4078.0397 - val_loss: 4043.9683\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 3958.7682 - val_loss: 3978.9997\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 3972.9115 - val_loss: 3955.5714\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 3959.0446 - val_loss: 3940.5484\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 3954.1515 - val_loss: 3936.3855\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 3977.3786 - val_loss: 3932.0859\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 3906.9426 - val_loss: 3930.3919\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 3975.7385 - val_loss: 3932.1672\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 4003.0744 - val_loss: 3933.1659\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 3952.7253 - val_loss: 3932.8414\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 3972.3676 - val_loss: 3934.2554\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 3952.0213 - val_loss: 3931.1075\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 3983.0196 - val_loss: 3929.8413\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 3963.8794 - val_loss: 3929.2836\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 3899.4556 - val_loss: 3928.4144\n",
      "Percent of variability explained by model: 1.35322173052\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF18',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 18744.2722 - val_loss: 19074.3203\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 18279.2641 - val_loss: 18718.4480\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 17938.3101 - val_loss: 18393.8900\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 17556.0541 - val_loss: 18022.5358\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 17171.9077 - val_loss: 17615.0990\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 16740.2670 - val_loss: 17142.5259\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 16288.3951 - val_loss: 16642.7629\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 15763.2628 - val_loss: 16102.7216\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 15222.8111 - val_loss: 15543.7195\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 14667.7533 - val_loss: 14980.4200\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 14172.3603 - val_loss: 14410.8022\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 13577.6773 - val_loss: 13831.7882\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 13110.6039 - val_loss: 13269.9711\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 12590.6777 - val_loss: 12750.5111\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 12191.1372 - val_loss: 12268.3314\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 11702.7405 - val_loss: 11840.9488\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 11330.9077 - val_loss: 11491.8811\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 11144.5220 - val_loss: 11219.3217\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 10999.1644 - val_loss: 11053.5681\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 10863.2864 - val_loss: 10950.5029\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 10824.9442 - val_loss: 10897.1107\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 10805.7023 - val_loss: 10877.2154\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 10784.7654 - val_loss: 10866.7357\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 10848.6033 - val_loss: 10866.2338\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 10803.9171 - val_loss: 10862.9972\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 10767.1857 - val_loss: 10860.8342\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 10842.3422 - val_loss: 10821.0203\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 10777.1996 - val_loss: 10787.2853\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 10977.6337 - val_loss: 10769.0943\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 10652.9033 - val_loss: 10740.3247\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 10696.7205 - val_loss: 10640.2055\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 10713.1484 - val_loss: 10606.6910\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 10908.9671 - val_loss: 10616.5042\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 10757.9798 - val_loss: 10526.5241\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 10861.3602 - val_loss: 10504.4291\n",
      "Percent of variability explained by model: 2.06880731388\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF27',35,5960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4768 samples, validate on 1192 samples\n",
      "Epoch 1/35\n",
      "4768/4768 [==============================] - 9s - loss: 10420.5704 - val_loss: 8756.7509\n",
      "Epoch 2/35\n",
      "4768/4768 [==============================] - 9s - loss: 10109.8713 - val_loss: 8375.7107\n",
      "Epoch 3/35\n",
      "4768/4768 [==============================] - 9s - loss: 9781.5750 - val_loss: 8090.5768\n",
      "Epoch 4/35\n",
      "4768/4768 [==============================] - 9s - loss: 9485.4790 - val_loss: 7809.4614\n",
      "Epoch 5/35\n",
      "4768/4768 [==============================] - 9s - loss: 9189.2579 - val_loss: 7515.7224\n",
      "Epoch 6/35\n",
      "4768/4768 [==============================] - 9s - loss: 8905.5076 - val_loss: 7205.1788\n",
      "Epoch 7/35\n",
      "4768/4768 [==============================] - 9s - loss: 8594.0850 - val_loss: 6881.0137\n",
      "Epoch 8/35\n",
      "4768/4768 [==============================] - 9s - loss: 8260.0268 - val_loss: 6493.9966\n",
      "Epoch 9/35\n",
      "4768/4768 [==============================] - 9s - loss: 7872.9390 - val_loss: 6102.5359\n",
      "Epoch 10/35\n",
      "4768/4768 [==============================] - 9s - loss: 7479.7144 - val_loss: 5731.1808\n",
      "Epoch 11/35\n",
      "4768/4768 [==============================] - 9s - loss: 7172.7298 - val_loss: 5399.8816\n",
      "Epoch 12/35\n",
      "4768/4768 [==============================] - 9s - loss: 6867.3108 - val_loss: 5107.6534\n",
      "Epoch 13/35\n",
      "4768/4768 [==============================] - 9s - loss: 6623.8801 - val_loss: 4859.7648\n",
      "Epoch 14/35\n",
      "4768/4768 [==============================] - 9s - loss: 6466.8666 - val_loss: 4663.6207\n",
      "Epoch 15/35\n",
      "4768/4768 [==============================] - 9s - loss: 6288.2106 - val_loss: 4522.0391\n",
      "Epoch 16/35\n",
      "4768/4768 [==============================] - 9s - loss: 6199.7686 - val_loss: 4437.9086\n",
      "Epoch 17/35\n",
      "4768/4768 [==============================] - 9s - loss: 6198.6030 - val_loss: 4396.7168\n",
      "Epoch 18/35\n",
      "4768/4768 [==============================] - 9s - loss: 6090.3164 - val_loss: 4373.9370\n",
      "Epoch 19/35\n",
      "4768/4768 [==============================] - 9s - loss: 6087.1651 - val_loss: 4363.8964\n",
      "Epoch 20/35\n",
      "4768/4768 [==============================] - 9s - loss: 6171.0217 - val_loss: 4367.5215\n",
      "Epoch 21/35\n",
      "4768/4768 [==============================] - 9s - loss: 6166.1833 - val_loss: 4363.5162\n",
      "Epoch 22/35\n",
      "4768/4768 [==============================] - 9s - loss: 6088.2428 - val_loss: 4359.9851\n",
      "Epoch 23/35\n",
      "4768/4768 [==============================] - 9s - loss: 6156.3785 - val_loss: 4361.3981\n",
      "Epoch 24/35\n",
      "4768/4768 [==============================] - 9s - loss: 6202.8644 - val_loss: 4363.3255\n",
      "Epoch 25/35\n",
      "4768/4768 [==============================] - 9s - loss: 6131.8864 - val_loss: 4356.7086\n",
      "Epoch 26/35\n",
      "4768/4768 [==============================] - 9s - loss: 6129.0710 - val_loss: 4354.1063\n",
      "Epoch 27/35\n",
      "4768/4768 [==============================] - 9s - loss: 6090.2526 - val_loss: 4348.2977\n",
      "Epoch 28/35\n",
      "4768/4768 [==============================] - 9s - loss: 6166.1622 - val_loss: 4342.8343\n",
      "Epoch 29/35\n",
      "4768/4768 [==============================] - 9s - loss: 6101.2057 - val_loss: 4339.2586\n",
      "Epoch 30/35\n",
      "4768/4768 [==============================] - 9s - loss: 6085.7709 - val_loss: 4343.8753\n",
      "Epoch 31/35\n",
      "4768/4768 [==============================] - 9s - loss: 6099.9631 - val_loss: 4331.3811\n",
      "Epoch 32/35\n",
      "4768/4768 [==============================] - 9s - loss: 6092.3739 - val_loss: 4333.0799\n",
      "Epoch 33/35\n",
      "4768/4768 [==============================] - 9s - loss: 6095.4406 - val_loss: 4327.4171\n",
      "Epoch 34/35\n",
      "4768/4768 [==============================] - 9s - loss: 6056.4980 - val_loss: 4330.3611\n",
      "Epoch 35/35\n",
      "4768/4768 [==============================] - 9s - loss: 6050.3559 - val_loss: 4319.3550\n",
      "Percent of variability explained by model: 1.16073776296\n"
     ]
    }
   ],
   "source": [
    "Train_and_save_DanQ_model_no_negative('ARF29',35,5960)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
